<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cray System Management Administration Guide on Cray System Management (CSM)</title>
    <link>/docs-csm/en-12/operations/</link>
    <description>Recent content in Cray System Management Administration Guide on Cray System Management (CSM)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Fri, 08 Jul 2022 03:30:23 +0000</lastBuildDate><atom:link href="/docs-csm/en-12/operations/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Added Hardware</title>
      <link>/docs-csm/en-12/operations/network/management_network/added_hardware/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/added_hardware/</guid>
      <description>Added Hardware Follow this procedure when new hardware is added to the system.
Procedure   Validate the SHCD.
The SHCD defines the topology of a Shasta system, this is needed when generating switch configurations.
Refer to Validate the SHCD.
  Generate the switch configuration file(s).
Refer to Generate Switch Configs.
  Check the differences between the generated configurations and the configurations on the system.
Refer to Validate Switch Configs.</description>
    </item>
    
    <item>
      <title>Apply Custom Switch Configuration 1.2</title>
      <link>/docs-csm/en-12/operations/network/management_network/apply_custom_config_1.2/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/apply_custom_config_1.2/</guid>
      <description>Apply Custom Switch Configuration CSM 1.2 Apply the backed up site connection configuration with a couple modifications. Since virtual routing and forwarding (VRF) is now used to separate customer traffic, the site ports and default routes must be added to that VRF.
Prerequisites   Access to the switches
  Custom switch configurations
 Backup Custom Configuration    Generated switch configurations already applied
 Apply Switch Configurations    Aruba apply configurations vrf attach Customer will be added to the port configuration that connects to the site.</description>
    </item>
    
    <item>
      <title>Apply Custom Switch Configurations For 1.0</title>
      <link>/docs-csm/en-12/operations/network/management_network/apply_custom_config_1.0/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/apply_custom_config_1.0/</guid>
      <description>Apply Custom Switch Configurations for CSM 1.0 Apply the backed up site connection configuration with a couple modifications. Since virtual routing and forwarding (VRF) is now used to separate customer traffic, the site ports and default routes must be added to that VRF.
Prerequisites  Access to the switches Custom switch configurations  Backup Custom Configurations   Generated switch configurations already applied  Apply Switch Configurations    Aruba apply configurations sw-spine-001# conf t interface 1/1/36 no shutdown description to:CANswitch_cfcanb6s1-31:from:sw-25g01_x3000u39-j36 ip address 10.</description>
    </item>
    
    <item>
      <title>Apply Switch Configurations</title>
      <link>/docs-csm/en-12/operations/network/management_network/apply_switch_configurations/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/apply_switch_configurations/</guid>
      <description>Apply Switch Configurations This process is generally straightforward and requires the user to copy and paste the generated switch configuration into the terminal.
All ports will be shutdown before applying switch configuration. If the port is in the SHCD and being used, it will be enabled when the configuration is applied.
There are some caveats that are mentioned below.
Prerequisites  Switch without any configuration  Wipe Management Switches   Generated switch configurations  Generate Switch Configuration    Aruba   Shutdown all ports.</description>
    </item>
    
    <item>
      <title>Gateway Testing</title>
      <link>/docs-csm/en-12/operations/network/gateway_testing/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/gateway_testing/</guid>
      <description>Gateway Testing With the introduction of BiCAN, service APIs are now available on one or more networks depending on who is allowed access to the services and from where. The services are accessed via three different ingress gateways using a token that can be retrieved from Keycloak.
This page describes how to run a set of tests that determine if the gateways are functioning properly. The gateway test will obtain an API token from Keycloak and then use that token to attempt to access a set of service APIs on one or more networks, as defined in the gateway test definition file (gateway-test-defn.</description>
    </item>
    
    <item>
      <title>Management Network 1.0 (1.2 Preconfig) To 1.2</title>
      <link>/docs-csm/en-12/operations/network/management_network/1.0_to_1.2_upgrade/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:28 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/management_network/1.0_to_1.2_upgrade/</guid>
      <description>Management Network 1.0 (1.2 Preconfig) to 1.2  Prerequisites Mellanox  Mellanox Manual Configuration   Dell  Dell Manual Configuration   Aruba Spine  Aruba Manual Configuration   Aruba Leaf and Leaf BMC  Prerequisites  System is already running with CANU-generated 1.0 configurations (1.2 Preconfig). Generated switch configurations for 1.2.  Generate Switch Configurations   CANU installed with version 1.1.11 or greater.  Run canu --version to see version.</description>
    </item>
    
    <item>
      <title>Domain Name Service (DNS) Overview</title>
      <link>/docs-csm/en-12/operations/network/dns/dns/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/dns/</guid>
      <description>Domain Name Service (DNS) Overview DNS Architecture This diagram shows how the various components of the DNS infrastructure interact.
DNS Components The DNS infrastructure is comprised of a number of components.
Unbound (cray-dns-unbound) Unbound is a caching DNS resolver which is also used as the primary DNS server.
The DNS records served by Unbound include system component names (xnames), node hostnames, and service names and these records are read from the cray-dns-unbound ConfigMap which is populated by cray-dns-unbound-manager.</description>
    </item>
    
    <item>
      <title>Enable Ncsd On UANs</title>
      <link>/docs-csm/en-12/operations/network/dns/enable_ncsd_on_uans/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/enable_ncsd_on_uans/</guid>
      <description>Enable ncsd on UANs Configure User Access Nodes (UANs) to start the ncsd service at boot time.
The nscd service is not currently enabled by default and systemd does not start it at boot time. There are two ways to start nscd on UAN nodes: manually starting the service or enabling the service in the UAN image. While restarting nscd manually has to be performed each time the UAN is rebooted, enabling nscd in the image only has to be done once.</description>
    </item>
    
    <item>
      <title>External DNS</title>
      <link>/docs-csm/en-12/operations/network/external_dns/external_dns/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/external_dns/</guid>
      <description>External DNS External DNS, along with the customer accessible networks CMN and CAN/CHN, PowerDNS, Border Gateway Protocol (BGP), and MetalLB, makes it simpler to access the HPE Cray EX API and system management services. Services are accessible directly from a laptop without needing to tunnel into a non-compute node (NCN) or override /etc/hosts settings. Some services may require a JSON Web Token (JWT) to access them, while others may require OAuth2 to login using a DC LDAP password.</description>
    </item>
    
    <item>
      <title>External DNS Csi Input Values</title>
      <link>/docs-csm/en-12/operations/network/external_dns/external_dns_csi_config_init_input_values/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/external_dns_csi_config_init_input_values/</guid>
      <description>External DNS CSI Input Values External DNS requires the system-name, site-domain, and cmn-external-dns values that are defined with the csi config init command. These values are used to customize the External DNS configuration during installation.
The system-name and site-domain values The system-name and site-domain values specified as part of the csi config init are used together in the system-name.site-domain format, creating the external domain for external hostnames for services accessible from the Customer Management Network (CMN).</description>
    </item>
    
    <item>
      <title>External DNS Failing To Discover Services Workaround</title>
      <link>/docs-csm/en-12/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/external_dns_failing_to_discover_services_workaround/</guid>
      <description>External DNS Failing to Discover Services Workaround Many external DNS issues can be worked around by directly connecting to the desired backend service. This can circumvent authentication and authorization protections, but it may be necessary to access specific services when mitigating critical issues.
Istio&amp;rsquo;s ingress gateway uses Gateway and VirtualService objects to configure how traffic is routed to backend services. Currently, there are three gateways supporting the externally accessible services. They are services-gateway, customer-admin-gateway and customer-user-gateway.</description>
    </item>
    
    <item>
      <title>Ingress Routing</title>
      <link>/docs-csm/en-12/operations/network/external_dns/ingress_routing/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/ingress_routing/</guid>
      <description>Ingress Routing Ingress routing to services via Istio&amp;rsquo;s ingress gateway is configured by VirtualService custom resource definitions (CRD). When using external hostnames, there needs to be a VirtualService CRD that matches the external hostname to the desired destination.
For example, the configuration below controls the ingress routing for prometheus.cmn.SYSTEM_DOMAIN_NAME:
ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus Example output:
NAME GATEWAYS HOSTS AGE cray-sysmgmt-health-prometheus [services/services-gateway] [prometheus.cmn.SYSTEM_DOMAIN_NAME] 22h ncn-w001# kubectl get vs -n sysmgmt-health cray-sysmgmt-health-prometheus -o yaml Example output:</description>
    </item>
    
    <item>
      <title>Manage The DNS Unbound Resolver</title>
      <link>/docs-csm/en-12/operations/network/dns/manage_the_dns_unbound_resolver/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/manage_the_dns_unbound_resolver/</guid>
      <description>Manage the DNS Unbound Resolver The unbound DNS instance is used to resolve names for the physical equipment on the management networks within the system, such as NCNs, UANs, switches, compute nodes, and more. This instance is accessible only within the HPE Cray EX system.
Check the Status of the cray-dns-unbound Pods Use the kubectl command to check the status of the pods:
ncn-w001# kubectl get -n services pods | grep unbound Example output:</description>
    </item>
    
    <item>
      <title>PowerDNS Configuration</title>
      <link>/docs-csm/en-12/operations/network/dns/powerdns_configuration/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/powerdns_configuration/</guid>
      <description>PowerDNS Configuration External DNS PowerDNS replaces the CoreDNS server that earlier versions of CSM used to provide External DNS services.
The cray-dns-powerdns-can-tcp and cray-dns-powerdns-can-udp LoadBalancer resources are configured to service external DNS requests using the IP address specified by the CSI --cmn-external-dns command line argument.
The CSI --system-name and --site-domain command line arguments are combined to form the subdomain used for External DNS.
Site setup In the following example, the IP address 10.</description>
    </item>
    
    <item>
      <title>PowerDNS Migration Guide</title>
      <link>/docs-csm/en-12/operations/network/dns/powerdns_migration/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/powerdns_migration/</guid>
      <description>PowerDNS Migration Guide The migration to PowerDNS as the authoritative DNS source and the introduction of Bifurcated CAN (Customer Access Network) will result in some changes to the node and service naming conventions.
DNS Record Naming Changes Fully qualified domain names will be introduced for all DNS records.
Canonical name: hostname.network-path.system-name.site-domain
 hostname - The hostname of the node or service network-path - The network path used to access the node  .</description>
    </item>
    
    <item>
      <title>Troubleshoot Common DNS Issues</title>
      <link>/docs-csm/en-12/operations/network/dns/troubleshoot_common_dns_issues/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/troubleshoot_common_dns_issues/</guid>
      <description>Troubleshoot Common DNS Issues The Domain Name Service (DNS) is part of an integrated infrastructure set designed to provide dynamic host discovery, addressing, and naming. There are several different place to look for troubleshooting as DNS interacts with Dynamic Host Configuration Protocol (DHCP), the Hardware Management Service (HMS), the System Layout Service (SLS), and the State Manager Daemon (SMD).
The information below describes what to check when experiencing issues with DNS.</description>
    </item>
    
    <item>
      <title>Troubleshoot Connectivity To Services With External Ip Addresses</title>
      <link>/docs-csm/en-12/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/troubleshoot_systems_not_provisioned_with_external_ip_addresses/</guid>
      <description>Troubleshoot Connectivity to Services with External IP addresses Systems that do not support CMN/CAN/CHN will not have services provisioned with external IP addresses on CMN/CAN/CHN. Kubernetes will report a &amp;lt;pending&amp;gt; status for the external IP address of the service experiencing connectivity issues.
If SSH access to a non-compute node (NCN) is available, it is possible to override resolution of external hostnames and forward local ports into the cluster for the cluster IP address of the corresponding service.</description>
    </item>
    
    <item>
      <title>Troubleshoot DNS Configuration Issues</title>
      <link>/docs-csm/en-12/operations/network/external_dns/troubleshoot_dns_configuration_issues/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/troubleshoot_dns_configuration_issues/</guid>
      <description>Troubleshoot DNS Configuration Issues Troubleshoot issues when DNS is not properly configured to delegate name resolution to the core DNS instance on a specific cluster. Although the CMN/CAN/CHN IP address may still be routable using the IP address directly, it may not work because Istio&amp;rsquo;s ingress gateway depends on the hostname (or SNI) to route traffic. For command line tools like cURL, using the &amp;ndash;resolve option to force correct resolution can be used to work around this issue.</description>
    </item>
    
    <item>
      <title>Troubleshoot PowerDNS</title>
      <link>/docs-csm/en-12/operations/network/dns/troubleshoot_powerdns/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dns/troubleshoot_powerdns/</guid>
      <description>Troubleshoot PowerDNS  List DNS zone contents PowerDNS logging Verify DNSSEC operation  Verify zones are being signed with the zone signing key Verify TSIG operation    List DNS zone contents The PowerDNS zone database is populated with data from two sources:
 The cray-powerdns-manager service creates the zones and DNS records based on data sourced from the System Layout Service (SLS). The external DNS records are populated by the cray-externaldns-external-dns service using data sourced from Kubernetes annotations and virtual service definitions.</description>
    </item>
    
    <item>
      <title>Update The Cmn-external-DNS Value Post-installation</title>
      <link>/docs-csm/en-12/operations/network/external_dns/update_the_cmn-external-dns_value_post-installation/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:27 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/external_dns/update_the_cmn-external-dns_value_post-installation/</guid>
      <description>Update the cmn-external-dns value post-installation By default, the services/cray-dns-powerdns-cmn-tcp and services/cray-dns-powerdns-cmn-udp services both share the same Customer Management Network (CMN) external IP address. This is defined by the cmn-external-dns value, which is specified during the csi config init input.
The IP address must be in the static range reserved in MetalLB&amp;rsquo;s cmn-static-pool subnet. Currently, this is the only CMN IP address that must be known external to the system, in order for external DNS to delegate the system-name.</description>
    </item>
    
    <item>
      <title>Bi-CAN Aruba/arista Configuration</title>
      <link>/docs-csm/en-12/operations/network/customer_accessible_networks/bi-can_arista_aruba_config/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_accessible_networks/bi-can_arista_aruba_config/</guid>
      <description>BI-CAN Aruba/Arista Configuration This is an example configuration of how to connect two Aruba spine switches to two Arista switches. This example is from a running system utilizing the bifurcated CAN feature offered in CSM 1.2.
Summary:
 Two Aruba 8325 switches running in a VSX cluster. Two Arista 7060CX2-32S switches running MLAG. The Arista switches are connected to the Slingshot/HSN network via static MLAG. The Aruba Spine switches are connected to the Arista switches with point-to-point OSPF links.</description>
    </item>
    
    <item>
      <title>CAN/cmn With Dual-spine Configuration</title>
      <link>/docs-csm/en-12/operations/network/customer_accessible_networks/dual_spine_configuration/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_accessible_networks/dual_spine_configuration/</guid>
      <description>CAN/CMN with Dual-Spine Configuration The Customer Access Network (CAN) and Customer Management Network (CMN) needs to be connected to both spines in a dual-spine configuration so that each spine can access the outside network. However, the NCNs should only have one default gateway. Therefore, the multi-active gateway protocol (MAGP) on the Mellanox spines can be used to create a virtual router gateway IP address that can direct to either of the spines, depending on the state of the spines.</description>
    </item>
    
    <item>
      <title>Connect To The Cmn And CAN</title>
      <link>/docs-csm/en-12/operations/network/customer_accessible_networks/connect_to_the_cmn_can/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_accessible_networks/connect_to_the_cmn_can/</guid>
      <description>Connect to the CMN and CAN How to connect to the CMN and CAN physically and via layer 3.
There are multiple ways to connect to the Customer Management Network (CMN) and Customer Access Network (CAN), both physically and via a layer 3 connection.
Physical Connection to the CMN and CAN The physical connection to the CMN and CAN is made via the load balancer or the spine switches. The uplink connection from the system to the customer network is achieved by using the highest numbered port(s).</description>
    </item>
    
    <item>
      <title>Connect To The HPE Cray Ex Environment</title>
      <link>/docs-csm/en-12/operations/network/connect_to_the_hpe_cray_ex_environment/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/connect_to_the_hpe_cray_ex_environment/</guid>
      <description>Connect to the HPE Cray EX Environment The HPE Cray EX Management Network (SMNet) has multiple separate physical and logical links that are used to segregate traffic.
The diagram below shows the available connections from within the SMNet, as well as the connections to the customer network:
There are multiple ways to connect to the HPE Cray EX environment. The various methods are described in the following table:
   Role Description     Administrative External customer network connection to the worker node&amp;rsquo;s hardware management and administrative port   Application node access External customer network connection to an Application Node   Customer Access Network (CAN) Customer connection to the CAN gateway to access the HPE Cray EX CAN    There are also several ways to physically connect to the nodes on the system.</description>
    </item>
    
    <item>
      <title>Create A Configuration Upgrade Plan</title>
      <link>/docs-csm/en-12/operations/network/create_a_csm_configuration_upgrade_plan/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/create_a_csm_configuration_upgrade_plan/</guid>
      <description>Create a CSM Configuration Upgrade Plan Creating an upgrade plan is unique and dependent on the requirements of the upgrade path. Some release versions of the network configuration require coupled upgrade of software to enable new software functionality, or bug fixes that may add time required to do the full upgrade.
For example, in CSM release 1.2, Aruba and Mellanox switches are being upgraded to newer code.
In this case and cases where configuration changes are extensive, consider taking the generated configurations after review and uploading them to the switches startup config prior to booting to new code to upgrade both configuration and software simultaneously.</description>
    </item>
    
    <item>
      <title>Customer Accessible Networks</title>
      <link>/docs-csm/en-12/operations/network/customer_accessible_networks/customer_accessible_networks/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_accessible_networks/customer_accessible_networks/</guid>
      <description>Customer Accessible Networks There are generally two networks accessible by devices outside of the CSM cluster. One network is for administrators managing the cluster and one is for users accessing user services provided by the cluster.
Customer Management Network The Customer Management Network (CMN) provides access from outside the customer network to administrative services and non-compute nodes (NCNs). This allows for the following:
 Administrator clients outside of the system:  Log in to NCNs.</description>
    </item>
    
    <item>
      <title>Default Ip Address Ranges</title>
      <link>/docs-csm/en-12/operations/network/default_ip_address_ranges/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/default_ip_address_ranges/</guid>
      <description>Default IP Address Ranges The initial installation of the system creates default networks with default settings and with no external exposure. These IP address default ranges ensure that no nodes in the system attempt to use the same IP address as a Kubernetes service or pod, which would result in undefined behavior that is extremely difficult to reproduce or debug.
The following table shows the default IP address ranges:
   Network IP Address Range     Kubernetes service network 10.</description>
    </item>
    
    <item>
      <title>DHCP</title>
      <link>/docs-csm/en-12/operations/network/dhcp/dhcp/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dhcp/dhcp/</guid>
      <description>DHCP The Dynamic Host Configuration Protocol (DHCP) service on the HPE Cray EX system uses the Internet Systems Consortium (ISC) Kea tool. Kea provides more robust management capabilities for DHCP servers.
For more information: https://www.isc.org/kea/.
The following improvements to the DHCP service are included:
 Persistent and resilient data store for DHCP leases in Postgres API access to manage DHCP Scalable pod that uses MetalLB instead of host networking Options for updates to HPE Cray EX management system IP addresses  DHCP Helper Workflow The DHCP-Helper uses the following workflow:</description>
    </item>
    
    <item>
      <title>Externally Exposed Services</title>
      <link>/docs-csm/en-12/operations/network/customer_accessible_networks/externally_exposed_services/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_accessible_networks/externally_exposed_services/</guid>
      <description>Externally Exposed Services The following services are exposed on one or more of the external networks (CMN, CAN, and CHN). Each of these services requires an IP address in the relevant subnets so they are reachable on that network. This IP address is allocated by the MetalLB component.
Services under Istio Ingress Gateway and OAuth2 Proxy Ingress share an ingress, so they all use the IP allocated to the Ingress.</description>
    </item>
    
    <item>
      <title>Metallb Peering With Arista Edge Router</title>
      <link>/docs-csm/en-12/operations/network/customer_accessible_networks/bi-can_arista_metallb_peering/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_accessible_networks/bi-can_arista_metallb_peering/</guid>
      <description>MetalLB Peering with Arista Edge Router This is an example configuration of how to connect a pair of Arista switches to MetalLB running inside of Kubernetes.
Prerequisites  Pair of Arista switches already connected to the high-speed network. Updated System Layout Service (SLS) file that has the CHN network configured.  Example Configuration Below is a snippet from an upgraded SLS.
&amp;#34;CHN&amp;#34;: { &amp;#34;Name&amp;#34;: &amp;#34;CHN&amp;#34;, &amp;#34;FullName&amp;#34;: &amp;#34;Customer High-Speed Network&amp;#34;, &amp;#34;IPRanges&amp;#34;: [ &amp;#34;10.</description>
    </item>
    
    <item>
      <title>Network</title>
      <link>/docs-csm/en-12/operations/network/network/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/network/</guid>
      <description>Network There are several different networks supported by the HPE Cray EX system. The following are the available internal and external networks, as well as the devices that connect to each network:
 Networks external to the system:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network All NCNs (worker, master, and storage) are connected ClusterStor System Management Unit (SMU) interfaces User Access Nodes (UANs)     System networks:  Customer Network (Data Center)  ncn-m001 BMC is connected by the customer network switch to the customer management network ClusterStor SMU interfaces User Access Nodes (UANs)   Hardware Management Network (HMN)  BMCs for Admin tasks Power distribution units (PDU) Keyboard/video/mouse (KVM)   Node Management Network (NMN)  All NCNs and compute nodes User Access Nodes (UANs)   ClusterStor Management Network  ClusterStor controller management interfaces of all ClusterStor components (SMU, Metadata Management Unit (MMU), and Scalable Storage Unit (SSU))   High-Speed Network (HSN), which connects the following devices:  Kubernetes worker nodes UANs ClusterStor controller data interfaces of all ClusterStor components (SMU, MMU, and SSU) There must be at least two NCNs whose BMCs are on the HMN.</description>
    </item>
    
    <item>
      <title>Troubleshoot Cmn Issues</title>
      <link>/docs-csm/en-12/operations/network/customer_accessible_networks/troubleshoot_cmn_issues/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/customer_accessible_networks/troubleshoot_cmn_issues/</guid>
      <description>Troubleshoot CMN issues Various connection points to check when using the CMN and how to fix any issues that arise.
The most frequent issue with the Customer Management Network (CMN) is trouble accessing IP addresses outside of the HPE Cray EX system from a node or pod inside the system.
The best way to resolve this issue is to try to ping an outside IP address from one of the NCNs other than ncn-m001, which has a direct connection that it can use instead of the Customer Management Network (CMN).</description>
    </item>
    
    <item>
      <title>Troubleshoot DHCP Issues</title>
      <link>/docs-csm/en-12/operations/network/dhcp/troubleshoot_dhcp_issues/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:26 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/dhcp/troubleshoot_dhcp_issues/</guid>
      <description>Troubleshoot DHCP Issues There are several things to check for when troubleshooting issues with Dynamic Host Configuration Protocol (DHCP) servers.
Incorrect DHCP IP Addresses One of the most common issues is when the DHCP IP addresses are not matching in the Domain Name Service (DNS).
Check to make sure cray-dhcp is not running in Kubernetes:
ncn-w001# kubectl get pods -A | grep cray-dhcp Example output:
services cray-dhcp-5f8c8767db-hg6ch 1/1 Running 0 35d If the cray-dhcp pod is running, use the following command to shut down the pod:</description>
    </item>
    
    <item>
      <title>Access To System Management Services</title>
      <link>/docs-csm/en-12/operations/network/access_to_system_management_services/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/network/access_to_system_management_services/</guid>
      <description>Access to System Management Services The standard configuration for System Management Services (SMS) is the containerized REST micro-service with a public API. All of the micro-services provide an HTTP interface and are collectively exposed through a single gateway URL. The API gateway for the system is available at a well known URL based on the domain name of the system. It acts as a single HTTPS endpoint for terminating Transport Layer Security (TLS) using the configured certificate authority.</description>
    </item>
    
    <item>
      <title>Rebalance Healthy Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/rebalance_healthy_etcd_clusters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/rebalance_healthy_etcd_clusters/</guid>
      <description>Rebalance Healthy etcd Clusters Rebalance the etcd clusters. The clusters need to be in a healthy state, and there needs to be the same number of pods running on each worker node for the etcd clusters to be balanced.
Restoring the balance of etcd clusters will help with the storage of Kubernetes cluster data.
Prerequisites  etcd clusters are in a healthy state. etcd clusters do not have the same number of pods on each worker node.</description>
    </item>
    
    <item>
      <title>Rebuild Unhealthy Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/rebuild_unhealthy_etcd_clusters/</guid>
      <description>Rebuild Unhealthy etcd Clusters Rebuild any cluster that does not have healthy pods by deleting and redeploying unhealthy pods. This procedure includes examples for rebuilding etcd clusters in the services namespace. This procedure must be used for each unhealthy cluster, not just the services used in the following examples.
This process also applies when etcd is not visible when running the kubectl get pods command.
A special use case is also included for the Content Projection Service (CPS) as the process for rebuilding the cluster is slightly different.</description>
    </item>
    
    <item>
      <title>Recover From Postgres Wal Event</title>
      <link>/docs-csm/en-12/operations/kubernetes/recover_from_postgres_wal_event/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/recover_from_postgres_wal_event/</guid>
      <description>Recover from Postgres WAL Event A WAL event can occur because of lag, network communication, or bandwidth issues. This can cause the PVC hosted by Ceph and mounted inside the container on /home/postgres/pgdata to fill and the database to stop running. If no database dump exists, the disk space issue needs to be fixed so that a dump can be taken. Then the dump can be restored to a newly created postgresql cluster.</description>
    </item>
    
    <item>
      <title>Repopulate Data In Etcd Clusters When Rebuilding Them</title>
      <link>/docs-csm/en-12/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/repopulate_data_in_etcd_clusters_when_rebuilding_them/</guid>
      <description>Repopulate Data in etcd Clusters When Rebuilding Them When an etcd cluster is not healthy, it needs to be rebuilt. During that process, the pods that rely on etcd clusters lose data. That data needs to be repopulated in order for the cluster to go back to a healthy state.
The following services need their data repopulated in the etcd cluster:
 Boot Orchestration Service (BOS) Boot Script Service (BSS) Content Projection Service (CPS) Compute Rolling Upgrade Service (CRUS) External DNS Firmware Action Service (FAS) HMS Notification Fanout Daemon (HMNFD) Mountain Endpoint Discovery Service (MEDS) River Endpoint Discovery Service (REDS)  Prerequisites An etcd cluster was rebuilt.</description>
    </item>
    
    <item>
      <title>Report The Endpoint Status For Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/report_the_endpoint_status_for_etcd_clusters/</guid>
      <description>Report the Endpoint Status for etcd Clusters Report etcd cluster end point status. The report includes a cluster&amp;rsquo;s endpoint, database size, and leader status.
This procedure provides the ability to view the etcd cluster endpoint status.
Prerequisites  This procedure requires root privileges. The etcd clusters are in a healthy state.  Procedure   Report the endpoint status for all etcd clusters in a namespace.
The following example is for the services namespace.</description>
    </item>
    
    <item>
      <title>Restore An Etcd Cluster From A Backup</title>
      <link>/docs-csm/en-12/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/restore_an_etcd_cluster_from_a_backup/</guid>
      <description>Restore an etcd Cluster from a Backup Use an existing backup of a healthy etcd cluster to restore an unhealthy cluster to a healthy state.
The commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.
 NOTE
Etcd Clusters can be restored using the automation script or the manual procedure below. The automation script follows the same steps as the manual procedure.</description>
    </item>
    
    <item>
      <title>Restore Bare-metal Etcd Clusters From An S3 Snapshot</title>
      <link>/docs-csm/en-12/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/restore_bare-metal_etcd_clusters_from_an_s3_snapshot/</guid>
      <description>Restore Bare-Metal etcd Clusters from an S3 Snapshot The etcd cluster that serves Kubernetes on master nodes is backed up every 10 minutes. These backups are pushed to Ceph Rados Gateway (S3).
Restoring the etcd cluster from backup is only meant to be used in a catastrophic scenario, whereby the Kubernetes cluster and master nodes are being rebuilt. This procedure shows how to restore the bare-metal etcd cluster from an Simple Storage Service (S3) snapshot.</description>
    </item>
    
    <item>
      <title>Restore Postgres</title>
      <link>/docs-csm/en-12/operations/kubernetes/restore_postgres/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/restore_postgres/</guid>
      <description>Restore Postgres Below are the service specific steps required to restore data to a Postgres cluster.
Restore Postgres Procedures by Service:
 Restore Postgres for Spire Restore Postgres for Keycloak Restore Postgres for VCS Restore Postgres for Capsules  Capsules Warehouse Server Capsules Dispatch Server    
Restore Postgres for Spire In the event that the Spire Postgres cluster is in a state that the cluster must be rebuilt and the data restored, the following procedures are recommended.</description>
    </item>
    
    <item>
      <title>Retrieve Cluster Health Information Using Kubernetes</title>
      <link>/docs-csm/en-12/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/retrieve_cluster_health_information_using_kubernetes/</guid>
      <description>Retrieve Cluster Health Information Using Kubernetes The kubectl CLI commands can be used to retrieve information about the Kubernetes cluster components.
Nodes Retrieve node status ncn# kubectl get nodes Example output:
NAME STATUS ROLES AGE VERSION ncn-m001 Ready control-plane,master 27h v1.20.13 ncn-m002 Ready control-plane,master 8d v1.20.13 ncn-m003 Ready control-plane,master 8d v1.20.13 ncn-w001 Ready &amp;lt;none&amp;gt; 8d v1.20.13 ncn-w002 Ready &amp;lt;none&amp;gt; 8d v1.20.13 ncn-w003 Ready &amp;lt;none&amp;gt; 8d v1.20.13 Pods Retrieve information about individual pods ncn# kubectl describe pod POD_NAME -n NAMESPACE_NAME Retrieve a list of all pods ncn# kubectl get pods -A Retrieve a list of healthy pods ncn# kubectl get pods -A | grep -E &amp;#39;Completed|Running&amp;#39; Retrieve a list of unhealthy pods   Option 1: List all pods that are not reported as Completed or Running.</description>
    </item>
    
    <item>
      <title>Tds Lower Cpu Requests</title>
      <link>/docs-csm/en-12/operations/kubernetes/tds_lower_cpu_requests/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/tds_lower_cpu_requests/</guid>
      <description>TDS Lower CPU Requests Systems with only three worker nodes (typically Test and Development Systems (TDS)) will encounter pod scheduling issues when worker nodes are taken out of the Kubernetes cluster to be upgraded.
For systems with only three worker nodes, execute the following script to reduce the CPU request for some services with high CPU requests, in order to allow critical upgrade-related services to be successfully scheduled on only two worker nodes:</description>
    </item>
    
    <item>
      <title>Troubleshoot Intermittent HTTP 503 Code Failures</title>
      <link>/docs-csm/en-12/operations/kubernetes/troubleshoot_intermittent_503s/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/troubleshoot_intermittent_503s/</guid>
      <description>Troubleshoot Intermittent HTTP 503 Code Failures There are cases where API calls or cray command invocations will intermittently return an HTTP 503 code, even when the backing service is up and functional. In the event that this occurs, the following steps may be useful to remediate the issue.
  Inspect the istio-ingressgateway pod logs:
ncn-m # kubectl -n istio-system logs -l app=istio-ingressgateway If the logs include TLS errors similar to the following, then proceed to the next step to restart the istio-ingressgateway pods.</description>
    </item>
    
    <item>
      <title>Troubleshoot Postgres Database</title>
      <link>/docs-csm/en-12/operations/kubernetes/troubleshoot_postgres_database/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/troubleshoot_postgres_database/</guid>
      <description>Troubleshoot Postgres Database This page contains general Postgres troubleshooting topics.
 The patronictl tool Database unavailable Database disk full Replication lagging Postgres status SyncFailed Cluster member missing Postgres leader missing  The patronictl tool The patronictl tool is used to call a REST API that interacts with Postgres databases. It handles a variety of tasks, such as listing cluster members and the replication status, configuring and restarting databases, and more.</description>
    </item>
    
    <item>
      <title>View Postgres Information For System Databases</title>
      <link>/docs-csm/en-12/operations/kubernetes/view_postgres_information_for_system_databases/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:25 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/view_postgres_information_for_system_databases/</guid>
      <description>View Postgres Information for System Databases Postgres uses SQL language to store and manage databases on the system. This procedure describes how to view and obtain helpful information about system databases, as well as the types of data being stored.
Prerequisites This procedure requires administrative privileges.
Procedure   Log in to the Postgres container.
ncn-w001# kubectl -n services exec -it cray-smd-postgres-0 -- bash Example output:
Defaulting container name to postgres.</description>
    </item>
    
    <item>
      <title>Check For And Clear Etcd Cluster Alarms</title>
      <link>/docs-csm/en-12/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/check_for_and_clear_etcd_cluster_alarms/</guid>
      <description>Check for and Clear etcd Cluster Alarms Check for any etcd cluster alarms and clear them as needed. An etcd cluster alarm must be manually cleared.
For example, a cluster&amp;rsquo;s database &amp;ldquo;NOSPACE&amp;rdquo; alarm is set when database storage space is no longer available. A subsequent defrag may free up database storage space, but writes to the database will continue to fail while the &amp;ldquo;NOSPACE&amp;rdquo; alarm is set.
Prerequisites  This procedure requires root privileges.</description>
    </item>
    
    <item>
      <title>Check The Health And Balance Of Etcd Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/check_the_health_and_balance_of_etcd_clusters/</guid>
      <description>Check the Health and Balance of etcd Clusters Check to see if all of the etcd clusters have healthy pods, are balanced, and have a healthy cluster database. There needs to be the same number of pods running on each worker node for the etcd clusters to be balanced. If the number of pods is not the same for each worker node, the cluster is not balanced.
Any clusters that do not have healthy pods will need to be rebuilt.</description>
    </item>
    
    <item>
      <title>Clear Space In An Etcd Cluster Database</title>
      <link>/docs-csm/en-12/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/clear_space_in_an_etcd_cluster_database/</guid>
      <description>Clear Space in an etcd Cluster Database Use this procedure to clear the etcd cluster NOSPACE alarm. Once it is set it will remain set. If needed, defrag the database cluster before clearing the NOSPACE alarm.
Defragging the database cluster and clearing the etcd cluster NOSPACE alarm will free up database space.
Prerequisites  This procedure requires root privileges The etcd clusters are in a healthy state  Procedure   Clear up space when the etcd database space has exceeded and has been defragged, but the NOSPACE alarm remains set.</description>
    </item>
    
    <item>
      <title>Configure Kubectl Credentials To Access The Kubernetes Apis</title>
      <link>/docs-csm/en-12/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/configure_kubectl_credentials_to_access_the_kubernetes_apis/</guid>
      <description>Configure kubectl Credentials to Access the Kubernetes APIs The credentials for kubectl are located in the admin configuration file on all non-compute node (NCN) master and worker nodes. They can be found at /etc/kubernetes/admin.conf for the root user. Use kubectl to access the Kubernetes cluster from a device outside the cluster.
For more information, refer to https://kubernetes.io/
Prerequisites This procedure requires administrative privileges and assumes that the device being used has:</description>
    </item>
    
    <item>
      <title>Containerd</title>
      <link>/docs-csm/en-12/operations/kubernetes/containerd/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/containerd/</guid>
      <description>Containerd Containerd is a daemonset that runs on the host. It is used to run containers on the Kubernetes platform.
/var/lib/containerd filling up In older versions of containerd, there are cases where the /var/lib/containerd directory fills up. In the event that this occurs, the following steps can be used to remediate the issue.
  Restart containerd on the NCN:
ncn-w001 # systemctl restart containerd Many times this will free up space in /var/lib/containerd &amp;ndash; if not proceed to Step 2.</description>
    </item>
    
    <item>
      <title>Create A Manual Backup Of A Healthy Etcd Cluster</title>
      <link>/docs-csm/en-12/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/create_a_manual_backup_of_a_healthy_etcd_cluster/</guid>
      <description>Create a Manual Backup of a Healthy etcd Cluster Manually create a backup of a healthy etcd cluster and check to see if the backup was created successfully.
Backups of healthy etcd clusters can be used to restore the cluster if it becomes unhealthy at any point.
The commands in this procedure can be run on any master node (ncn-mXXX) or worker node (ncn-wXXX) on the system.
Prerequisites A healthy etcd cluster is available on the system.</description>
    </item>
    
    <item>
      <title>Determine If Pods Are Hitting Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/determine_if_pods_are_hitting_resource_limits/</guid>
      <description>Determine if Pods are Hitting Resource Limits Determine if a pod is being CPU throttled or hitting its memory limits (OOMKilled). Use the detect_cpu_throttling.sh script to determine if any pods are being CPU throttled, and check the Kubernetes events to see if any pods are hitting a memory limit.
IMPORTANT: The presence of CPU throttling does not always indicate a problem, but if a service is being slow or experiencing latency issues, this procedure can be used to evaluate if it is not performing well as a result of CPU throttling.</description>
    </item>
    
    <item>
      <title>Disaster Recovery For Postgres</title>
      <link>/docs-csm/en-12/operations/kubernetes/disaster_recovery_postgres/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/disaster_recovery_postgres/</guid>
      <description>Disaster Recovery for Postgres In the event that the Postgres cluster has failed to the point that it must be recovered and there is no dump available to restore the data, a full service specific disaster recovery is needed.
Below are the service specific steps required to cleanup any existing resources, redeploy the resources, and repopulate the data.
Disaster recovery procedures by service:
 Restore HSM (Hardware State Manger) Postgres without a Backup Restore SLS (System Layout Service) Postgres without a Backup Restore Spire Postgres without a Backup Restore Keycloak Postgres without a Backup Restore console Postgres  Restore Keycloak Postgres without a backup The following procedures are required to rebuild the automatically populated contents of Keycloak&amp;rsquo;s PostgreSQL database if the database has been lost and recreated.</description>
    </item>
    
    <item>
      <title>Increase Kafka Pod Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/increase_kafka_pod_resource_limits/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/increase_kafka_pod_resource_limits/</guid>
      <description>Increase Kafka Pod Resource Limits For larger scale systems, the Kafka resource limits may need to be increased. See Increase Pod Resource Limits for details on how to increase limits.
Increase Kafka Resource Limits Example
For a 1500 compute node system, increasing the cpu count to 6 and memory limits to 128G should be adequate.</description>
    </item>
    
    <item>
      <title>Increase Pod Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/increase_pod_resource_limits/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/increase_pod_resource_limits/</guid>
      <description>Increase Pod Resource Limits Increase the appropriate resource limits for pods after determining if a pod is being CPU throttled or OOMKilled.
Return Kubernetes pods to a healthy state with resources available.
Prerequisites  kubectl is installed. The names of the pods hitting their resource limits are known. See Determine if Pods are Hitting Resource Limits.  Procedure   Determine the current limits of a pod.
In the example below, cray-hbtd-etcd-8r2scmpb58 is the POD_ID being used.</description>
    </item>
    
    <item>
      <title>Kubernetes</title>
      <link>/docs-csm/en-12/operations/kubernetes/kubernetes/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/kubernetes/</guid>
      <description>Kubernetes The system management components are broken down into a series of micro-services. Each service is independently deployable, fine-grained, and uses lightweight protocols. As a result, the system&amp;rsquo;s micro-services are modular, resilient, and can be updated independently. Services within this architecture communicate via REST APIs.
About Kubernetes Kubernetes is a portable and extensible platform for managing containerized workloads and services. Kubernetes serves as a micro-services platform on the system that facilitates application deployment, scaling, and management.</description>
    </item>
    
    <item>
      <title>Kubernetes Networking</title>
      <link>/docs-csm/en-12/operations/kubernetes/kubernetes_networking/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/kubernetes_networking/</guid>
      <description>Kubernetes Networking Every Kubernetes pod has an IP address in the pod network that is reachable within the cluster. The system uses the weave-net plugin for inter-node communication.
Access services from outside the cluster All services with a REST API must be accessed from outside the cluster using the Istio Ingress Gateway. This gateway can be accessed using a URL in the following format:
https://api.cmn.SYSTEM-NAME_DOMAIN-NAME https://api.can.SYSTEM-NAME_DOMAIN-NAME https://api.chn.SYSTEM-NAME_DOMAIN-NAME The API requests then get routed to the appropriate node running that service.</description>
    </item>
    
    <item>
      <title>Kubernetes Storage</title>
      <link>/docs-csm/en-12/operations/kubernetes/kubernetes_storage/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/kubernetes_storage/</guid>
      <description>Kubernetes Storage Data belonging to micro-services in the management cluster is managed through persistent storage, which provides reliable and resilient data protection for containers running in the Kubernetes cluster.
The backing storage for this service is currently provided by JBOD disks that are spread across several nodes of the management cluster. These node disks are managed by Ceph, and are exposed to containers in the form of persistent volumes.</description>
    </item>
    
    <item>
      <title>Pod Resource Limits</title>
      <link>/docs-csm/en-12/operations/kubernetes/pod_resource_limits/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:24 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/pod_resource_limits/</guid>
      <description>Pod Resource Limits Kubernetes uses resource requests and Quality of Service (QoS) for scheduling pods. Resource requests can be provided explicitly for pods and containers, whereas pod QoS is implicit, based on the resource requests and limits of the containers in the pod. There are three types of QoS:
 Guaranteed: All containers in a pod have explicit memory and CPU resource requests and limits. For each resource, the limit equals the request.</description>
    </item>
    
    <item>
      <title>About Etcd</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_etcd/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_etcd/</guid>
      <description>About etcd The system uses etcd for storing all of its cluster data. It is an open source database that is excellent for maintaining the state of Kubernetes. Failures in the etcd cluster at the heart of Kubernetes will cause a failure of Kubernetes. To mitigate this risk, the system is deployed with etcd on dedicated disks and with a specific configuration to optimize Kubernetes workloads. The system also provides additional etcd cluster(s) as necessary to help maintain an operational state of services.</description>
    </item>
    
    <item>
      <title>About Kubectl</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_kubectl/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_kubectl/</guid>
      <description>About kubectl kubectl is a CLI that can be used to run commands against a Kubernetes cluster. The format of the kubectl command is shown below:
ncn# kubectl COMMAND RESOURCE_TYPE RESOURCE_NAME FLAGS An example of using kubectl to retrieve information about a pod is shown below:
ncn# kubectl get pod POD_NAME1 POD_NAME2 kubectl is installed by default on the non-compute node (NCN) image. To learn more about kubectl, refer to https://kubernetes.</description>
    </item>
    
    <item>
      <title>About Kubernetes Taints And Labels</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_kubernetes_taints_and_labels/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_kubernetes_taints_and_labels/</guid>
      <description>About Kubernetes Taints and Labels Kubernetes labels control node affinity, which is the property of pods that attracts them to a set of nodes. On the other hand, Kubernetes taints enable a node to repel a set of pods. In addition, pods can have tolerances for taints to allow them to run on nodes with certain taints.
Taints are controlled with the kubectl taint nodes command, while node labels for various nodes can be customized with a configmap that contains the desired values.</description>
    </item>
    
    <item>
      <title>About Postgres</title>
      <link>/docs-csm/en-12/operations/kubernetes/about_postgres/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/about_postgres/</guid>
      <description>About Postgres The system uses PostgreSQL (known as Postgres) as a database solution. Postgres databases use SQL language to store and manage databases on the system.
To learn more about Postgres, see https://www.postgresql.org/docs/.
The Patroni tool can be used to manage and maintain information in a Postgres database. It handles tasks such as listing cluster members and the replication status, configuring and restarting databases, and more. For more information about this tool, refer to Troubleshoot Postgres Database.</description>
    </item>
    
    <item>
      <title>Backups For Etcd-operator Clusters</title>
      <link>/docs-csm/en-12/operations/kubernetes/backups_for_etcd-operator_clusters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/backups_for_etcd-operator_clusters/</guid>
      <description>Backups for etcd-operator Clusters Backups are periodically created for etcd clusters. These backups are stored in the Ceph Rados Gateway (S3). Not all services are backed up automatically. Services that are not backed up automatically will need to be manually rediscovered if the cluster is unhealthy.
Clusters with Automated Backups The following services are backed up (daily, one week&amp;rsquo;s worth of backups retained) as part of the automated solution:
 Boot Orchestration Service (BOS) Boot Script Service (BSS) Compute Rolling Upgrade Service (CRUS) External DNS Firmware Action Service (FAS) User Access Service (UAS)  Run the following command on any master node (ncn-mXXX) or the first worker node (ncn-w001) to list the backups for a specific project.</description>
    </item>
    
    <item>
      <title>Customize An Image Root Using IMS</title>
      <link>/docs-csm/en-12/operations/image_management/customize_an_image_root_using_ims/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/customize_an_image_root_using_ims/</guid>
      <description>Customize an Image Root Using IMS The Image Management Service (IMS) customization workflow sets up a temporary image customization environment within a Kubernetes pod and mounts the image to be customized in that environment. A system administrator then makes the desired changes to the image root within the customization environment.
Afterwards, the IMS customization workflow automatically copies the NCN CA public key to /etc/cray/ca/certificate_authority.crt within the image root being customized, in order to enable secure communications between NCNs and client nodes.</description>
    </item>
    
    <item>
      <title>Delete Or Recover Deleted IMS Content</title>
      <link>/docs-csm/en-12/operations/image_management/delete_or_recover_deleted_ims_content/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/delete_or_recover_deleted_ims_content/</guid>
      <description>Delete or Recover Deleted IMS Content The Image Management System (IMS) manages user supplied SSH public Keys, customizable image recipes, images, and IMS jobs that are used to build or customize images. In previous versions of IMS, deleting an IMS public key, recipe, or image resulted in that item being permanently deleted. Additionally, IMS recipes and images store linked artifacts in the Simple Storage Service (S3) datastore. These artifacts are referenced by the IMS recipe and image records.</description>
    </item>
    
    <item>
      <title>Image Management</title>
      <link>/docs-csm/en-12/operations/image_management/image_management/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/image_management/</guid>
      <description>Image Management The Image Management Service (IMS) uses the open source Kiwi-NG tool to build image roots from compressed Kiwi image descriptions. These compressed Kiwi image descriptions are referred to as &amp;ldquo;recipes.&amp;rdquo; Kiwi-NG builds images based on a variety of different Linux distributions, specifically SUSE, RHEL, and their derivatives. Kiwi image descriptions must follow the Kiwi development schema. More information about the development schema and the Kiwi-NG tool can be found in the documentation: https://doc.</description>
    </item>
    
    <item>
      <title>Image Management Workflows</title>
      <link>/docs-csm/en-12/operations/image_management/image_management_workflows/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/image_management_workflows/</guid>
      <description>Image Management Workflows Overview of how to create an image and how to customize and image.
The following workflows are intended to be high-level overviews of image management tasks. These workflows depict how services interact with each other during image management and help to provide a quicker and deeper understanding of how the system functions.
The workflows in this section include:
 Create a New Image Customize an Image  Create a New Image Use Case: The system administrator creates an image root from a customized recipe.</description>
    </item>
    
    <item>
      <title>Import An External Image To IMS</title>
      <link>/docs-csm/en-12/operations/image_management/import_external_image_to_ims/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/import_external_image_to_ims/</guid>
      <description>Import an External Image to IMS The Image Management Service (IMS) is typically used to build images from IMS recipes and customize Images that are already known to IMS. However, it is sometimes the case that an image is built using a mechanism other than by IMS and needs to be added to IMS. In these cases, the following procedure can be used to add this external image to IMS and upload the image&amp;rsquo;s artifact(s) to the Simple Storage Service (S3).</description>
    </item>
    
    <item>
      <title>Kubernetes And Bare Metal Etcd Certificate Renewal</title>
      <link>/docs-csm/en-12/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/kubernetes/cert_renewal_for_kubernetes_and_bare_metal_etcd/</guid>
      <description>Kubernetes and Bare Metal EtcD Certificate Renewal As part of the installation, Kubernetes generates certificates for the required subcomponents. This document will help walk through the process of renewing the certificates.
IMPORTANT:
 Depending on the version of Kubernetes, the command may or may not reside under the alpha category. Use kubectl certs --help and kubectl alpha certs --help to determine this. The overall command syntax is the same; the only difference is whether or not the command structure includes alpha.</description>
    </item>
    
    <item>
      <title>Upload And Register An Image Recipe</title>
      <link>/docs-csm/en-12/operations/image_management/upload_and_register_an_image_recipe/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:23 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/upload_and_register_an_image_recipe/</guid>
      <description>Upload and Register an Image Recipe Download and expand recipe archives from S3 and IMS. Modify and upload a recipe archive, and then register that recipe archive with IMS.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCNs) and include the following deployment:  cray-ims, the Image Management Service (IMS)   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    
    <item>
      <title>Adjust Hm Collector Resource Limits And Requests</title>
      <link>/docs-csm/en-12/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hmcollector/adjust_hmcollector_resource_limits_requests/</guid>
      <description>Adjust HM Collector resource limits and requests  Resource Limit Tuning Guidance Customize cray-hms-hmcollector resource limits and requests in customizations.yaml Redeploy cray-hms-hmcollector with new resource limits and requests  Resource Limit Tuning Guidance Inspect current resource usage in the cray-hms-hmcollector pod View resource usage of the containers in the cray-hms-hmcollector pod:
ncn-m001# kubectl -n services top pod -l app.kubernetes.io/name=cray-hms-hmcollector --containers POD NAME CPU(cores) MEMORY(bytes) cray-hms-hmcollector-7c5b797c5c-zxt67 istio-proxy 187m 275Mi cray-hms-hmcollector-7c5b797c5c-zxt67 cray-hms-hmcollector 4398m 296Mi The default resource limits for the cray-hms-hmcollector container are:</description>
    </item>
    
    <item>
      <title>Build A New UAN Image Using The Default Recipe</title>
      <link>/docs-csm/en-12/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/build_a_new_uan_image_using_the_default_recipe/</guid>
      <description>Build a New UAN Image Using the Default Recipe Build or rebuild the User Access Node (UAN) image using either the default UAN image or image recipe. Both of these are supplied by the UAN product stream installer.
 Prerequisites Overview Remove Slingshot Diagnostics RPM From Default UAN Recipe Build the UAN Image Automatically Using IMS Build the UAN Image By Customizing It Manually  Prerequisites  Both the Cray Operation System (COS) and UAN product streams must be installed.</description>
    </item>
    
    <item>
      <title>Build An Image Using IMS Rest Service</title>
      <link>/docs-csm/en-12/operations/image_management/build_an_image_using_ims_rest_service/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/build_an_image_using_ims_rest_service/</guid>
      <description>Build an Image Using IMS REST Service Create an image root from an IMS recipe.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. System management services (SMS) are running in a Kubernetes cluster on non-compute nodes (NCN) and include the following deployments:  cray-ims, the Image Management Service (IMS) cray-nexus, the Nexus repository manager service   The NCN Certificate Authority (CA) public key has been properly installed into the CA cache for this system.</description>
    </item>
    
    <item>
      <title>Configure IMS To Validate Rpms</title>
      <link>/docs-csm/en-12/operations/image_management/configure_ims_to_validate_rpms/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/configure_ims_to_validate_rpms/</guid>
      <description>Configure IMS to Validate RPMs Configuring the Image Management Service (IMS) to validate the GPG signatures of RPMs during IMS Build operations involves the following two steps:
  Create and update IMS to use a new Kiwi-NG Image with the Signing Keys embedded.
 NOTE: The default IMS Kiwi-NG Image is already configured with the signing keys needed to validate HPE and SuSE RPMs and repositories.
   Update IMS Recipes to require GPG verification of RPMs, repositories, or both.</description>
    </item>
    
    <item>
      <title>Convert Tgz Archives To SqUAShfs Images</title>
      <link>/docs-csm/en-12/operations/image_management/convert_tgz_archives_to_squashfs_images/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/convert_tgz_archives_to_squashfs_images/</guid>
      <description>Convert TGZ Archives to SquashFS Images If customizing a pre-built image root archive compressed as a .txz or other non-SquashFS format, convert the image root to SquashFS and upload the SquashFS archive to S3.
The steps in this section only apply if the image root is not in SquashFS format.
Prerequisites There is a pre-built image that is not currently in SquashFS format.
Procedure   Locate the image root to be converted to SquashFS.</description>
    </item>
    
    <item>
      <title>Create UAN Boot Images</title>
      <link>/docs-csm/en-12/operations/image_management/create_uan_boot_images/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/image_management/create_uan_boot_images/</guid>
      <description>Create UAN Boot Images Update configuration management Git repository to match the installed version of the UAN product. Then use that updated configuration to create UAN boot images and a BOS session template.
This is the overall workflow for preparing UAN images for booting UANs:
 Clone the UAN configuration Git repository and create a branch based on the branch imported by the UAN installation. Update the configuration content and push the changes to the newly created branch.</description>
    </item>
    
    <item>
      <title>HPE Pdu Admin Procedures</title>
      <link>/docs-csm/en-12/operations/hpe_pdu/hpe_pdu_admin_procedures/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hpe_pdu/hpe_pdu_admin_procedures/</guid>
      <description>HPE PDU Admin Procedures The following procedures are used to manage the HPE Power Distribution Unit (PDU):
 Verify PDU vendor Connect to HPE PDU web interface HPE PDU initial set-up Update HPE PDU firmware Change HPE PDU user passwords Update Vault credentials Discover HPE PDU after upgrading CSM   IMPORTANT: Because of the polling method used to process sensor data from the HPE PDU, telemetry data may take up to six minutes to refresh; this includes the outlet status reported by the Hardware State Manager (HSM).</description>
    </item>
    
    <item>
      <title>Lock And Unlock Management Nodes</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/lock_and_unlock_management_nodes/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/lock_and_unlock_management_nodes/</guid>
      <description>Lock and Unlock Management Nodes The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes, NCNs, and their BMCs are also not locked by default. The administrator must lock the NCNs and their BMCs to prevent unwanted actions from affecting these nodes.
This section only covers using locks with the Hardware State Manager (HSM). For more information on ignoring nodes, refer to the following sections:
 Firmware Action Service (FAS): See Ignore Node within FAS.</description>
    </item>
    
    <item>
      <title>Manage Component Groups</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/manage_component_groups/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/manage_component_groups/</guid>
      <description>Manage Component Groups The creation, deletion, and modification of groups is enabled by the Hardware State Manager (HSM) APIs.
The following is an example group that contains the optional fields tags and exclusiveGroup:
{ &amp;#34;label&amp;#34; : &amp;#34;blue&amp;#34;, &amp;#34;description&amp;#34; : &amp;#34;blue node group&amp;#34;, &amp;#34;tags&amp;#34; : [ &amp;#34;tag1&amp;#34;, &amp;#34;tag2&amp;#34; ], &amp;#34;members&amp;#34; : { &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;, &amp;#34;x0c0s0b0n1&amp;#34;, &amp;#34;x0c0s0b1n0&amp;#34;, &amp;#34;x0c0s0b1n1&amp;#34; ] }, &amp;#34;exclusiveGroup&amp;#34; : &amp;#34;colors&amp;#34; } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.</description>
    </item>
    
    <item>
      <title>Manage Component Partitions</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/manage_component_partitions/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/manage_component_partitions/</guid>
      <description>Manage Component Partitions The creation, deletion, and modification of partitions is enabled by the Hardware State Manager (HSM) APIs.
The following is an example partition that contains the optional tags field:
{ &amp;quot;name&amp;quot; : &amp;quot;partition 1&amp;quot;, &amp;quot;description&amp;quot; : &amp;quot;partition 1&amp;quot;, &amp;quot;tags&amp;quot; : [ &amp;quot;tag2&amp;quot; ], &amp;quot;members&amp;quot; : { &amp;quot;ids&amp;quot; : [ &amp;quot;x0c0s0b0n0&amp;quot;, &amp;quot;x0c0s0b0n1&amp;quot;, &amp;quot;x0c0s0b1n0&amp;quot; ] }, } Troubleshooting: If the Cray CLI has not been initialized, the CLI commands will not work.</description>
    </item>
    
    <item>
      <title>Manage Hms Locks</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/manage_hms_locks/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/manage_hms_locks/</guid>
      <description>Manage HMS Locks This section describes how to check the status of a lock, disable reservations, and repair reservations. The disable and repair operations only affect the ability to make reservations on hardware devices.
Some of the common scenarios an admin might encounter when working with the Hardware State Manager (HSM) Locking API are also described.
Check Lock Status Use the following command to verify if a component name (xname) is locked or not.</description>
    </item>
    
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres Database From Backup</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_from_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres Database from Backup This procedure can be used to restore the HSM Postgres database from a previously taken backup. This can be a manual backup created by the Create a Backup of the HSM Postgres Database procedure, or an automatic backup created by the cray-smd-postgresql-db-backup Kubernetes cronjob.
Prerequisites   Healthy System Layout Service (SLS). Recovered first if also affected.
  Healthy HSM Postgres Cluster.</description>
    </item>
    
    <item>
      <title>Restore Hardware State Manager (HSM) Postgres Without An Existing Backup</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/restore_hsm_postgres_without_a_backup/</guid>
      <description>Restore Hardware State Manager (HSM) Postgres without an Existing Backup This procedure is intended to repopulate HSM in the event when no Postgres backup exists.
Prerequisite   Healthy System Layout Service (SLS). Recovered first if also affected.
  Healthy HSM service.
Verify all 3 HSM postgres replicas are up and running:
ncn# kubectl -n services get pods -l cluster-name=cray-smd-postgres Example output:
NAME READY STATUS RESTARTS AGE cray-smd-postgres-0 3/3 Running 0 18d cray-smd-postgres-1 3/3 Running 0 18d cray-smd-postgres-2 3/3 Running 0 18d   Procedure   Re-run the HSM loader job.</description>
    </item>
    
    <item>
      <title>Set BMC Management Roles</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/set_bmc_management_role/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:22 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/set_bmc_management_role/</guid>
      <description>Set BMC Management Roles The ability to ignore non-compute nodes (NCNs) is turned off by default. Management nodes and NCNs are also not locked by default. The administrator must lock the NCNs and their BMCs to prevent unwanted actions from affecting these nodes. To more easily identify the BMCs that are associated with the management nodes, they need to be marked with the Management role in the Hardware State Manager (HSM), just like their associated nodes.</description>
    </item>
    
    <item>
      <title>Add A Switch To The HSM Database</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/add_a_switch_to_the_hsm_database/</guid>
      <description>Add a Switch to the HSM Database Manually add a switch to the Hardware State Manager (HSM) database. Switches need to be in the HSM database in order to update their firmware with the Firmware Action Service (FAS).
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Add the switch to the HSM database.
The --rediscover-on-update true flag forces HSM to discover the switch.</description>
    </item>
    
    <item>
      <title>Add An NCN To The HSM Database</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/add_an_ncn_to_the_hsm_database/</guid>
      <description>Add an NCN to the HSM Database This procedure details how to customize the bare-metal non-compute node (NCN) on a system and add the NCN to the Hardware State Manager (HSM) database.
The examples in this procedure use ncn-w0003-nmn as the Customer Access Node (CAN). Use the correct CAN for the system.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system. The initial software installation is complete.</description>
    </item>
    
    <item>
      <title>Component Group Members</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_group_members/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_group_members/</guid>
      <description>Component Group Members The members object in the group definition has additional actions available for managing the members after the group has been created.
The following is an example of group members:
{ &amp;#34;ids&amp;#34; : [ &amp;#34;x0c0s0b0n0&amp;#34;,&amp;#34;x0c0s0b0n1&amp;#34;,&amp;#34;x0c0s0b1n0&amp;#34; ] } Retrieve Group Members Retrieve just the members array for a group:
ncn-m# cray hsm groups members list GROUP_LABEL Retrieve only the members of a group that are also in a specific partition:</description>
    </item>
    
    <item>
      <title>Component Groups And Partitions</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_groups_and_partitions/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_groups_and_partitions/</guid>
      <description>Component Groups and Partitions The Hardware State Manager (HSM) provides the group and partition services. Both are means of grouping (also known as labeling) system components that are tracked by HSM. Components include the nodes, blades, controllers, and more on a system.
There is no limit to the number of members a group or partition contains. The only limitation is that all members must be actual members of the system. The HSM needs to know that the components exist.</description>
    </item>
    
    <item>
      <title>Component Memberships</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_memberships/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_memberships/</guid>
      <description>Component Memberships Memberships are a read-only resource that is generated automatically by changes to groups and partitions. Each component in /hsm/v2/State/Components is represented. Filter options are available to prune the list, or a specific component name (xname) can be given. All groups and the partition (if any) of each component are listed.
At this point in time, only information about node components is needed. The --type node filter option is used in the commands below to retrieve information about node memberships only.</description>
    </item>
    
    <item>
      <title>Component Partition Members</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/component_partition_members/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/component_partition_members/</guid>
      <description>Component Partition Members The members object in the partition definition has additional actions available for managing the members after the partition has been created.
The following is an example of partition members:
{ &amp;quot;ids&amp;quot; : [ &amp;quot;x0c0s0b0n0&amp;quot;,&amp;quot;x0c0s0b0n1&amp;quot;,&amp;quot;x0c0s0b1n0&amp;quot;,&amp;quot;x0c0s0b1n1&amp;quot; ] } Retrieve Partition Members Retrieving members of a partition is very similar to how group members are retrieved and modified. No filtering options are available in partitions. However, there are partition and group filtering parameters for the /hsm/v2/State/Components and /hsm/v2/memberships collections, with both essentially working the same way.</description>
    </item>
    
    <item>
      <title>Create A Backup Of The HSM Postgres Database</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/create_a_backup_of_the_hsm_postgres_database/</guid>
      <description>Create a Backup of the HSM Postgres Database Perform a manual backup of the contents of the Hardware State Manager (HSM) Postgres database. This backup can be used to restore the contents of the HSM Postgres database at a later point in time using the Restore HSM Postgres from Backup procedure.
Prerequisites   Healthy HSM Postgres Cluster.
Use patronictl list on the HSM Postgres cluster to determine the current state of the cluster, and a healthy cluster will look similar to the following:</description>
    </item>
    
    <item>
      <title>Hardware Management Services (hms) Locking Api</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hardware_management_services_hms_locking_api/</guid>
      <description>Hardware Management Services (HMS) Locking API The locking feature is a part of the Hardware State Manager (HSM) API. The locking API enables administrators to lock components on the system. Locking components ensures other system actors, such as administrators or running services, cannot perform a firmware update with the Firmware Action Service (FAS) or a power state change with the Cray Advanced Platform Monitoring and Control (CAPMC). Locks only constrain FAS and CAPMC from each other and help ensure that a firmware update action will not be interfered with by a request to power off the device through CAPMC.</description>
    </item>
    
    <item>
      <title>Hardware State Manager (HSM)</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager/</guid>
      <description>Hardware State Manager (HSM) The Hardware State Manager (HSM) monitors and interrogates hardware components in the HPE Cray EX system, tracking hardware state and inventory information, and making it available via REST queries and message bus events when changes occur.
In the CSM 0.9.3 release, v1 of the HSM API has begun its deprecation process in favor of the new HSM v2 API. Refer to the HSM API documentation for more information on the changes.</description>
    </item>
    
    <item>
      <title>Hardware State Manager (HSM) State And Flag Fields</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hardware_state_manager_hsm_state_and_flag_fields/</guid>
      <description>Hardware State Manager (HSM) State and Flag Fields HSM manages important information for hardware components in the system. Administrators can use the data returned by HSM to learn about the state of the system. To do so, it is critical that the State and Flag fields are understood, and the next steps to take are known when viewing output returned by HSM commands. It is also beneficial to understand what services can cause State or Flag changes in HSM.</description>
    </item>
    
    <item>
      <title>HSM Roles And Subroles</title>
      <link>/docs-csm/en-12/operations/hardware_state_manager/hsm_roles_and_subroles/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/hardware_state_manager/hsm_roles_and_subroles/</guid>
      <description>HSM Roles and Subroles The Hardware State Manager (HSM) contains several pre-defined roles and subroles that can be assigned to components and used to target specific hardware devices.
Roles and subroles assignments come from the System Layout Service (SLS) and are applied by HSM when a node is discovered.
HSM Roles The following is a list of all pre-defined roles:
 Management Compute Application Service System Storage  The Management role refers to NCNs and will generally have the Master, Worker, or Storage subrole assigned.</description>
    </item>
    
    <item>
      <title>Upload BMC Recovery Firmware Into Tftp Server</title>
      <link>/docs-csm/en-12/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:21 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/upload_olympus_bmc_recovery_firmware_into_tftp_server/</guid>
      <description>Upload BMC Recovery Firmware into TFTP Server cray-upload-recovery-images is a utility for uploading the BMC recovery files for ChassisBMCs, NodeBMCs, and RouterBMCs to be served by the cray-tftp service. The tool uses the cray CLI (fas, artifacts) and cray-tftp to download the S3 recovery images (as remembered by FAS), then upload them into the PVC that is used by cray-tftp. cray-upload-recovery-images should be run on every system.
Prerequisites  Cray System Management (CSM) software is installed.</description>
    </item>
    
    <item>
      <title>FAS Admin Procedures</title>
      <link>/docs-csm/en-12/operations/firmware/fas_admin_procedures/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_admin_procedures/</guid>
      <description>FAS Admin Procedures 
Procedures for leveraging the Firmware Action Service (FAS) CLI to manage firmware.
Topics  Warning for Non-Compute Nodes (NCNs) Ignore Nodes within FAS Override an Image for an Update Check for New Firmware Versions with a Dry-Run Load Firmware from Nexus Load Firmware from RPM or ZIP file   Warning for Non-Compute Nodes (NCNs) NCNs and their BMCs should be locked with the HSM locking API to ensure they are not unintentionally updated by FAS.</description>
    </item>
    
    <item>
      <title>FAS Cli</title>
      <link>/docs-csm/en-12/operations/firmware/fas_cli/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_cli/</guid>
      <description>FAS CLI This section describes the basic capabilities of the Firmware Action Service (FAS) CLI commands. These commands can be used to manage firmware for system hardware supported by FAS. Refer to the prerequisites section before proceeding to any of the sections for the supported operations.
The following CLI operations are described:
 Prerequisites Actions  Execute an action  Procedure   Abort an action  Procedure   Describe an action  Interpreting output Procedure  Get high level summary Get details of action Get details of operation       Snapshots  Create a snapshot  Procedure   List snapshots  Procedure   View snapshots  Procedure     Update a firmware image  Procedure   FAS loader commands  Loader status Load firmware from Nexus Load individual RPM or ZIP into FAS Display results of loader run Delete loader run data    Prerequisites The Cray command line interface (CLI) tool is initialized and configured on the system.</description>
    </item>
    
    <item>
      <title>FAS Filters</title>
      <link>/docs-csm/en-12/operations/firmware/fas_filters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_filters/</guid>
      <description>FAS Filters FAS uses five primary filters for actions and snapshots to determine what operations to create. The filters are listed below:
 Selection Filters - Determine what operations will be created. The following selection filters are available:  stateComponentFilter targetFilter inventoryHardwareFilter imageFilter   Command Filters - Determine how the operations will be executed. The following command filters are available:  command    All filters are logically connected with AND logic.</description>
    </item>
    
    <item>
      <title>FAS Recipes</title>
      <link>/docs-csm/en-12/operations/firmware/fas_recipes/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_recipes/</guid>
      <description>FAS Recipes  NOTE: This is a collection of various FAS recipes for performing updates. For step by step directions and commands, see FAS Use Cases.
 The following example JSON files are useful to reference when updating specific hardware components. In all of these examples, the overrideDryrun field will be set to false; set them to true to perform a live update.
When updating an entire system, walk down the device hierarchy component type by component type, starting first with routers (switches), proceeding to chassis, and then finally to nodes.</description>
    </item>
    
    <item>
      <title>FAS Use Cases</title>
      <link>/docs-csm/en-12/operations/firmware/fas_use_cases/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/fas_use_cases/</guid>
      <description>FAS Use Cases Use the Firmware Action Service (FAS) to update the firmware on supported hardware devices. Each procedure includes the prerequisites and example recipes required to update the firmware.
When updating an entire system, walk down the device hierarchy component type by component type, starting first with Routers (switches), proceeding to Chassis, and then finally to Nodes. While this is not strictly necessary, it does help eliminate confusion.
Refer to FAS Filters for more information on the content used in the example JSON files.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Failing To Connect To A Console</title>
      <link>/docs-csm/en-12/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/troubleshoot_conman_failing_to_connect_to_a_console/</guid>
      <description>Troubleshoot ConMan Failing to Connect to a Console There are many reasons that ConMan may not be able to connect to a specific console. This procedure outlines several things to check that may impact the connectivity with a console.
Prerequisites This procedure requires administrative privileges.
Procedure Note: this procedure has changed since the CSM 0.9 release.
  Find the cray-console-operator pod.
ncn# OP_POD=$(kubectl get pods -n services \  -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;) ncn# echo $OP_POD Example output:</description>
    </item>
    
    <item>
      <title>Update Firmware With FAS</title>
      <link>/docs-csm/en-12/operations/firmware/update_firmware_with_fas/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/update_firmware_with_fas/</guid>
      <description>Update Firmware with FAS If FAS has not yet been installed, firmware for NCNs can be updated manually without FAS. See Updating Firmware without FAS.
The Firmware Action Service (FAS) provides an interface for managing firmware versions of Redfish-enabled hardware in the system. FAS interacts with the Hardware State Managers (HSM), device data, and image data in order to update firmware.
Reset Gigabyte node BMC to factory defaults if having problems with ipmitool, problems using Redfish, or when flashing procedures fail.</description>
    </item>
    
    <item>
      <title>Updating BMC Firmware And Bios For NCN-m001</title>
      <link>/docs-csm/en-12/operations/firmware/updating_firmware_m001/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/updating_firmware_m001/</guid>
      <description>Updating BMC Firmware and BIOS for ncn-m001 Retrieve the model name and firmware image required to update an HPE or Gigabyte ncn-m001 node.
 NOTE:
 On HPE nodes, the BMC Firmware is iLO 5 and BIOS is System ROM. The commands in the procedure must be run on ncn-m001.   Prerequisites The following information is needed:
 IP Address of ncn-m001 BMC IP Address of ncn-m001 Root password for ncn-m001 BMC  Find the Model Name Use one of the following commands to find the model name for the node type in use.</description>
    </item>
    
    <item>
      <title>Updating BMC Firmware And Bios For NCNs Without FAS</title>
      <link>/docs-csm/en-12/operations/firmware/updating_firmware_without_fas/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:20 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/firmware/updating_firmware_without_fas/</guid>
      <description>Updating BMC Firmware and BIOS for NCNs without FAS  NOTE
 On HPE nodes, the BMC firmware is iLO 5 and BIOS is System ROM. The commands in the procedure must be run on ncn-m001. This procedure should only be used if FAS is not available, such as during initial CSM install.    Prerequisites Obtain the required firmware Flash the firmware  Gigabyte NCNs HPE NCNs  Using the ilorest command Using the iLO GUI      Prerequisites The following information is needed:</description>
    </item>
    
    <item>
      <title>Access Compute Node Logs</title>
      <link>/docs-csm/en-12/operations/conman/access_compute_node_logs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/access_compute_node_logs/</guid>
      <description>Access Compute Node Logs This procedure shows how the ConMan utility can be used to retrieve compute node logs.
Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator pod.
Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
Procedure Note: this procedure has changed since the CSM 0.9 release.
  Log on to a Kubernetes master or worker node.</description>
    </item>
    
    <item>
      <title>Access Console Log Data Via The System Monitoring Framework (smf)</title>
      <link>/docs-csm/en-12/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/access_console_log_data_via_the_system_monitoring_framework_smf/</guid>
      <description>Access Console Log Data Via the System Monitoring Framework (SMF) Console log data is collected by SMF and can be queried through the Kibana UI or Elasticsearch. Each line of the console logs are an individual record in the SMF database.
Prerequisites This procedure requires the Kibana service to be up and running on a non-compute node (NCN).
Procedure   Determine the external domain name by running the following command on any NCN:</description>
    </item>
    
    <item>
      <title>Conman</title>
      <link>/docs-csm/en-12/operations/conman/conman/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/conman/</guid>
      <description>ConMan ConMan is a tool used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.
ConMan runs on the system as a containerized service. It runs in a set of Docker containers within Kubernetes pods named cray-console-operator and cray-console-node. Node console logs are stored locally within the cray-console-node pods in the /var/log/conman/ directory, as well as being collected by the System Monitoring Framework (SMF).</description>
    </item>
    
    <item>
      <title>Disable Conman After The System Software Installation</title>
      <link>/docs-csm/en-12/operations/conman/disable_conman_after_system_software_installation/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/disable_conman_after_system_software_installation/</guid>
      <description>Disable ConMan After the System Software Installation The ConMan utility is enabled by default. The first procedure provides instructions for disabling it after the system software has been installed, and the second procedure provides instructions on how to later re-enable it.
Prerequisites This procedure requires administrative privileges.
Disable Procedure Note: this procedure has changed since the CSM 0.9 release.
  Log on to a Kubernetes master or worker node.</description>
    </item>
    
    <item>
      <title>Establish A Serial Connection To NCNs</title>
      <link>/docs-csm/en-12/operations/conman/establish_a_serial_connection_to_ncns/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/establish_a_serial_connection_to_ncns/</guid>
      <description>Establish a Serial Connection to NCNs The ConMan pod can be used to establish a serial console connection with each non-compute node (NCN) in the system.
In the scenario of a power down or reboot of an NCN worker, one must first determine if any cray-console pods are running on that NCN. It is important to move cray-console pods to other worker nodes before rebooting or powering off a worker node to minimize disruption in console logging.</description>
    </item>
    
    <item>
      <title>Log In To A Node Using Conman</title>
      <link>/docs-csm/en-12/operations/conman/log_in_to_a_node_using_conman/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/log_in_to_a_node_using_conman/</guid>
      <description>Log in to a Node Using ConMan This procedure shows how to connect to the node&amp;rsquo;s Serial Over Lan (SOL) via ConMan.
Prerequisites The user performing this procedure needs to have access permission to the cray-console-operator and cray-console-node pods.
Procedure Note: this procedure has changed since the CSM 0.9 release.
  Log on to a Kubernetes master or worker node.
  Find the cray-console-operator pod.
ncn# OP_POD=$(kubectl get pods -n services \  -o wide|grep cray-console-operator|awk &amp;#39;{print $1}&amp;#39;) ncn# echo $OP_POD Example output:</description>
    </item>
    
    <item>
      <title>Manage Node Consoles</title>
      <link>/docs-csm/en-12/operations/conman/manage_node_consoles/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/manage_node_consoles/</guid>
      <description>Manage Node Consoles ConMan is used for connecting to remote consoles and collecting console logs. These node logs can then be used for various administrative purposes, such as troubleshooting node boot issues.
ConMan runs on the system in a set of containers within Kubernetes pods named cray-console-operator and cray-console-node.
The cray-console-operator and cray-console-node pods determine which nodes they should monitor by checking with the Hardware State Manager (HSM) service. They do this once when they starts.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Asking For Password On SSH Connection</title>
      <link>/docs-csm/en-12/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/troubleshoot_conman_asking_for_password_on_ssh_connection/</guid>
      <description>Troubleshoot ConMan Asking for Password on SSH Connection If ConMan starts to ask for a password when there is an SSH connection to the node on liquid-cooled hardware, that usually indicates there is a problem with the SSH key that was established on the node BMC. The key may have been replaced or overwritten on the hardware.
Use this procedure to renew or reinstall the SSH key on the BMCs.</description>
    </item>
    
    <item>
      <title>Troubleshoot Conman Blocking Access To A Node BMC</title>
      <link>/docs-csm/en-12/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:19 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/conman/troubleshoot_conman_blocking_access_to_a_node_bmc/</guid>
      <description>Troubleshoot ConMan Blocking Access to a Node BMC Disable ConMan if it is blocking access to a node by other means. ConMan runs on the system as a containerized service, and it is enabled by default. However, the use of ConMan to connect to a node blocks access to that node by other Serial over LAN (SOL) utilities or by a virtual KVM.
For information about how ConMan works, see ConMan.</description>
    </item>
    
    <item>
      <title>Configure The Cray Command Line Interface (cray Cli)</title>
      <link>/docs-csm/en-12/operations/configure_cray_cli/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configure_cray_cli/</guid>
      <description>Configure the Cray Command Line Interface (cray CLI) The cray command line interface (CLI) is a framework created to integrate all of the system management REST APIs into easily usable commands.
Procedures in the CSM installation workflow use the cray CLI to interact with multiple services. The cray CLI configuration needs to be initialized for the Linux account, and the Keycloak user running the procedure needs to be authorized. This section describes how to initialize the cray CLI for use by a user and how to authorize that user.</description>
    </item>
    
    <item>
      <title>Update A CFS Configuration</title>
      <link>/docs-csm/en-12/operations/configuration_management/update_a_cfs_configuration/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/update_a_cfs_configuration/</guid>
      <description>Update a CFS Configuration Modify a Configuration Framework Service (CFS) configuration by specifying the JSON of the configuration and its layers. Use the cray cfs configurations update command, similar to creating a configuration.
Prerequisites  A CFS configuration has been created. The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Add and/or remove the configuration layers from an existing JSON configuration file.</description>
    </item>
    
    <item>
      <title>Update The Privacy Settings For Gitea Configuration Content Repositories</title>
      <link>/docs-csm/en-12/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/update_the_privacy_settings_for_gitea_configuration_content_repositories/</guid>
      <description>Update the Privacy Settings for Gitea Configuration Content Repositories Change the visibility of Gitea configuration content repositories from public to private. All Cray-provided repositories are created as private by default.
Procedure   Log in to the Version Control Service (VCS) as the crayvcs user.
Use the following URL to access the VCS web interface:
https://vcs.SYSTEM-NAME.DOMAIN-NAME   Navigate to the cray organization.
https://vcs.SYSTEM-NAME.DOMAIN-NAME/vcs/cray   Select the repository title for each repository listed on the page.</description>
    </item>
    
    <item>
      <title>Use A Custom Ansible.cfg File</title>
      <link>/docs-csm/en-12/operations/configuration_management/use_a_custom_ansible-cfg_file/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/use_a_custom_ansible-cfg_file/</guid>
      <description>Use a Custom ansible.cfg File The Configuration Framework Service (CFS) allows for flexibility with the Ansible Execution Environment (AEE) by allowing for changes to included ansible.cfg file. When installed, CFS imports a custom ansible.cfg file into the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.
Administrators who want to make changes to the ansible.cfg file on a per-session or system-wide basis can upload a new file to a new ConfigMap and direct CFS to use their file.</description>
    </item>
    
    <item>
      <title>Use A Specific Inventory In A Configuration Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/use_a_specific_inventory_in_a_configuration_session/</guid>
      <description>Use a Specific Inventory in a Configuration Session A special repository can be added to a Configuration Framework Service (CFS) configuration to help with certain scenarios, specifically when developing Ansible plays for use on the system. A static inventory often changes along with the Ansible content, and CFS users may need to test different configuration values simultaneously and not be forced to use the global additionalInventoryUrl.
Therefore, an additional_inventory mapping can be added to the CFS configuration.</description>
    </item>
    
    <item>
      <title>Vcs Branching Strategy</title>
      <link>/docs-csm/en-12/operations/configuration_management/vcs_branching_strategy/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/vcs_branching_strategy/</guid>
      <description>VCS Branching Strategy Individual products import configuration content (Ansible plays, roles, and more) into a repository in the Version Control Service (VCS) through their installation process. Typically, this repository exists in the cray organization in VCS and its name has the format [product name]-config-management.
The import branch of the product is considered &amp;ldquo;pristine content&amp;rdquo; and is added to VCS in a read-only branch. This step is taken to ensure the future updates of the product&amp;rsquo;s configuration content can be based on a clean branch, and that upgrades can proceed without merging issues.</description>
    </item>
    
    <item>
      <title>Version Control Service (vcs)</title>
      <link>/docs-csm/en-12/operations/configuration_management/version_control_service_vcs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/version_control_service_vcs/</guid>
      <description>Version Control Service (VCS) The Version Control Service (VCS) includes a web interface for repository management, pull requests, and a visual view of all repositories and organizations. The following URL is for the VCS web interface:
https://vcs.SHASTA_CLUSTER_DNS_NAME
Cloning a VCS repository On cluster nodes, the VCS service can be accessed through the gateway. VCS credentials for the crayvcs user are required before cloning a repository (see VCS administrative user below).</description>
    </item>
    
    <item>
      <title>View Configuration Session Logs</title>
      <link>/docs-csm/en-12/operations/configuration_management/view_configuration_session_logs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/view_configuration_session_logs/</guid>
      <description>View Configuration Session Logs Logs for the individual steps of a session are available via the kubectl log command for each container of a Configuration Framework Service (CFS) session. Refer to Configuration Sessions for more info about these containers.
To find the name of the Kubernetes pod that is running the CFS session:
ncn-mw# kubectl get pods --no-headers -o custom-columns=&amp;#34;:metadata.name&amp;#34; -n services -l cfsession=example Store the returned pod name as the CFS_POD_NAME variable for future use:</description>
    </item>
    
    <item>
      <title>Write Ansible Code For CFS</title>
      <link>/docs-csm/en-12/operations/configuration_management/write_ansible_code_for_cfs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:18 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/write_ansible_code_for_cfs/</guid>
      <description>Write Ansible Code for CFS HPE Cray provides Ansible plays and roles for software products deemed necessary for the system to function. Customers are free to write their own Ansible plays and roles to augment what HPE Cray provides or implement new features. Basic knowledge of Ansible is needed to write plays and roles. The information below includes recommendations and best practices for writing and running Ansible code on the system successfully with the Configuration Framework Service (CFS).</description>
    </item>
    
    <item>
      <title>Enable Ansible Profiling</title>
      <link>/docs-csm/en-12/operations/configuration_management/enable_ansible_profiling/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/enable_ansible_profiling/</guid>
      <description>Enable Ansible Profiling Ansible tasks and playbooks can be profiled in order to determine execution times and single out poor performance in runtime. The default Configuration Framework Service (CFS) ansible.cfg in the cfs-default-ansible-cfg ConfigMap does not enable these profiling tools. If profiling tools are desired, modify the default Ansible configuration file to enable them.
Procedure   Edit the cfs-default-ansible-cfg ConfigMap.
ncn# kubectl edit cm cfs-default-ansible-cfg -n services   Uncomment the indicated line by removing the # character from the beginning of the line.</description>
    </item>
    
    <item>
      <title>Git Operations</title>
      <link>/docs-csm/en-12/operations/configuration_management/git_operations/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/git_operations/</guid>
      <description>Git Operations Use the git command to manage repository content in the Version Control Service (VCS).
Once a repository is cloned, the git command line tool is available to interact with a repository from VCS. The git command is used for making commits, creating new branches, and pushing new branches, tags, and commits to the remote repository stored in VCS.
When pushing changes to the VCS server using the crayvcs user, input the password retrieved from the Kubernetes secret as the credentials.</description>
    </item>
    
    <item>
      <title>Manage Multiple Inventories In A Single Location</title>
      <link>/docs-csm/en-12/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/manage_multiple_inventories_in_a_single_location/</guid>
      <description>Manage Multiple Inventories in a Single Location Many configuration layers may be present in a single configuration for larger systems that configure multiple Cray products. When values for each of these layers need to be customized, it can be tedious to override values in each of the respective repositories. The CFS additionalInventoryUrl option allows for static inventory files to be automatically added to the hosts directory of each configuration layer before it is applied by Ansible.</description>
    </item>
    
    <item>
      <title>Set Limits For A Configuration Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/set_limits_for_a_configuration_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/set_limits_for_a_configuration_session/</guid>
      <description>Set Limits for a Configuration Session The configuration layers and session hosts can be limited when running a Configuration Framework Service (CFS) session.
Limit CFS Session Hosts Subsets of nodes can be targeted in the inventory when running CFS sessions, which is useful specifically when running a session with dynamic inventory. Use the CFS --ansible-limit option when creating a session to apply the limits. The option directly corresponds to the --limit option offered by ansible-playbook, and can be used to specify hosts, groups, or combinations of them with patterns.</description>
    </item>
    
    <item>
      <title>Set The Ansible.cfg For A Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/set_the_ansible-cfg_for_a_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/set_the_ansible-cfg_for_a_session/</guid>
      <description>Set the ansible.cfg for a Session View and update the Ansible configuration used by the Configuration Framework Service (CFS).
Ansible configuration is available through the ansible.cfg file. See the Configuring Ansible external documentation for more information about what values can be set.
CFS provides a default ansible.cfg file in the cfs-default-ansible-cfg Kubernetes ConfigMap in the services namespace.
To view the ansible.cfg file:
ncn# kubectl get cm -n services cfs-default-ansible-cfg \ -o json | jq -r &amp;#39;.</description>
    </item>
    
    <item>
      <title>Specifying Hosts And Groups</title>
      <link>/docs-csm/en-12/operations/configuration_management/specifying_hosts_and_groups/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/specifying_hosts_and_groups/</guid>
      <description>Specifying Hosts and Groups When using the Configuration Framework Service (CFS), there are many steps where users may need to specify the hosts that CFS should configure. This can be done by specifying individual hosts, or groups of hosts. There are several places where a user may need to provide this information, particularly groups, and depending on where this information is provided, the behavior can change greatly.
Inventories CFS has multiple options for generating inventories, but regardless of which option is used, the information is then converted into an Ansible inventory/hosts file.</description>
    </item>
    
    <item>
      <title>Target Ansible Tasks For Image Customization</title>
      <link>/docs-csm/en-12/operations/configuration_management/target_ansible_tasks_for_image_customization/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/target_ansible_tasks_for_image_customization/</guid>
      <description>Target Ansible Tasks for Image Customization The Configuration Framework Service (CFS) enables Ansible playbooks to run against both running nodes and images. See the &amp;ldquo;Use Cases&amp;rdquo; header in the Configuration Management section for more information about image customization and when it should be used.
CFS uses the cray_cfs_image variable to distinguish between node personalization (running on live nodes) and image customization (configuring an image prior to boot). When this variable is set to true, it indicates that the CFS session is an image customization type and the playbook is targeting an image.</description>
    </item>
    
    <item>
      <title>Track The Status Of A Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/track_the_status_of_a_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/track_the_status_of_a_session/</guid>
      <description>Track the Status of a Session A configuration session can be a long-running process, and depends on many system factors, as well as the number of configuration layers and Ansible tasks that are run in each layer. The Configuration Framework Service (CFS) provides the session status through the session metadata to allow for tracking progress and session state.
To view the session status of a session named example, use the following command:</description>
    </item>
    
    <item>
      <title>Troubleshoot Ansible Play Failures In CFS Sessions</title>
      <link>/docs-csm/en-12/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/troubleshoot_ansible_play_failures_in_cfs_sessions/</guid>
      <description>Troubleshoot Ansible Play Failures in CFS Sessions View the Kubernetes logs for a Configuration Framework Service (CFS) pod in an error state to determine whether the error resulted from the CFS infrastructure or from an Ansible play that was run by a specific configuration layer in a CFS session.
Use this procedure to obtain important triage information for Ansible plays being called by CFS.
Prerequisites  A configuration session exists for CFS.</description>
    </item>
    
    <item>
      <title>Troubleshoot CFS Session Failing To Complete</title>
      <link>/docs-csm/en-12/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:17 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/troubleshoot_cfs_session_failing_to_complete/</guid>
      <description>Troubleshoot CFS Session Failing to Complete Troubleshoot issues where Configuration Framework Service (CFS) sessions/pods fail and Ansible hangs. These issues can be resolved by modifying Ansible to produce less output.
Prerequisites A CFS session or pod is failing to complete, and the Ansible logs are not showing progress or completion.
The following is an example of the error causing Ansible to hang:
PLAY [Compute] ***************************************************************** META: ran handlers META: ran handlers META: ran handlers PLAY [Compute] ***************************************************************** Using module file /usr/lib/python3.</description>
    </item>
    
    <item>
      <title>Configuration Management Of System Components</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_management_of_system_components/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_management_of_system_components/</guid>
      <description>Configuration Management of System Components The configuration of individual system components is managed with the cray cfs components command. The Configuration Framework Service (CFS) contains a database of the configuration state of available hardware known to the Hardware State Manager (HSM). When new nodes are added to the HSM database, a CFS Hardware Sync Agent enters the component into the CFS database with a null state of configuration.
Administrators are able to set a desired CFS configuration for each component, and the CFS Batcher ensures the desired configuration state and the current configuration state match.</description>
    </item>
    
    <item>
      <title>Configuration Management With The CFS Batcher</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_management_with_the_cfs_batcher/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_management_with_the_cfs_batcher/</guid>
      <description>Configuration Management with the CFS Batcher Creating configuration sessions with the Configuration Framework Service (CFS) enables remote execution for configuring live nodes and boot images prior to booting. CFS also provides its Batcher component for configuration management of registered system components. The CFS Batcher periodically examines the aggregated configuration state of registered components and schedules CFS sessions against those that have not been configured to their desired state. The frequency of scheduling, the maximum number of components to schedule in the same CFS session, and the expiration time for scheduling less than full sessions are configurable.</description>
    </item>
    
    <item>
      <title>Configuration Sessions</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_sessions/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_sessions/</guid>
      <description>Configuration Sessions Once configurations have been created with the required layers and values set in the configuration repositories (or the additional inventory repository), create a Configuration Framework Session (CFS) session to apply the configuration to the targets.
Sessions are created via the Cray CLI or through the CFS REST API. A session stages Ansible inventory (whether dynamic, static, or image customization), launches Ansible Execution Environments (AEE) in order for each configuration layer in the service mesh, tears down the environments as required, and reports the session status to the CFS API.</description>
    </item>
    
    <item>
      <title>Create A CFS Configuration</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_a_cfs_configuration/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_a_cfs_configuration/</guid>
      <description>Create a CFS Configuration Create a Configuration Framework Service (CFS) configuration, which contains an ordered list of layers. Each layer is defined by a Git repository clone URL, a Git commit, a name, and the path in the repository to an Ansible playbook to execute.
Prerequisites  The Cray command line interface (CLI) tool is initialized and configured on the system.  Procedure   Create a JSON file to hold data about the CFS configuration.</description>
    </item>
    
    <item>
      <title>Create A CFS Session With Dynamic Inventory</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_a_cfs_session_with_dynamic_inventory/</guid>
      <description>Create a CFS Session with Dynamic Inventory A Configuration Framework Service (CFS) session using dynamic inventory is used to configure live nodes. To create a CFS session using the default dynamic inventory, simply provide a session name and the name of the configuration to apply:
ncn# cray cfs sessions create --name example \ --configuration-name configurations-example Example output:
{ &amp;#34;ansible&amp;#34;: { &amp;#34;config&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;limit&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;verbosity&amp;#34;: 0 }, &amp;#34;configuration&amp;#34;: { &amp;#34;limit&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;configurations-example&amp;#34; }, &amp;#34;name&amp;#34;: &amp;#34;example&amp;#34;, &amp;#34;status&amp;#34;: { &amp;#34;artifacts&amp;#34;: [], &amp;#34;session&amp;#34;: { &amp;#34;status&amp;#34;: &amp;#34;pending&amp;#34;, &amp;#34;succeeded&amp;#34;: &amp;#34;none&amp;#34; } }, &amp;#34;tags&amp;#34;: {}, &amp;#34;target&amp;#34;: { &amp;#34;definition&amp;#34;: &amp;#34;dynamic&amp;#34;, &amp;#34;groups&amp;#34;: null } } Add the --target-definition dynamic parameter to the create command to explicitly define the inventory type to be dynamic.</description>
    </item>
    
    <item>
      <title>Create An Image Customization CFS Session</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_an_image_customization_cfs_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_an_image_customization_cfs_session/</guid>
      <description>Create an Image Customization CFS Session A configuration session that is meant to customize image roots tracked by the Image Management Service (IMS) can be created using the --target-definition image option. This option will instruct the Configuration Framework Service (CFS) to prepare the image IDs specified and assign them to the groups specified in Ansible inventory. IMS will then provide SSH connection information to each image root that CFS will use to configure Ansible.</description>
    </item>
    
    <item>
      <title>Create And Populate A Vcs Configuration Repository</title>
      <link>/docs-csm/en-12/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/create_and_populate_a_vcs_configuration_repository/</guid>
      <description>Create and Populate a VCS Configuration Repository Create a new repository in the VCS and populate it with content for site customizations in a custom Configuration Framework Service (CFS) configuration layer.
Prerequisites  The Version Control Service (VCS) login credentials for the crayvcs user are set up. See the &amp;ldquo;VCS Administrative User&amp;rdquo; heading in Version Control Service (VCS) for more information.  Procedure   Create the empty repository in VCS.</description>
    </item>
    
    <item>
      <title>Customize Configuration Values</title>
      <link>/docs-csm/en-12/operations/configuration_management/customize_configuration_values/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/customize_configuration_values/</guid>
      <description>Customize Configuration Values In general, most systems will require some customization from the default values provided by HPE Cray products. As stated in the previous section, these changes cannot be made on the pristine product branches that are imported during product installation and upgrades. Changes can only be made in Git branches that are based on the pristine branches.
Changing or overriding default values should be done in accordance with Ansible best practices (see the external Ansible best practices guide) and variable precedence (see the external Ansible variable guide) in mind.</description>
    </item>
    
    <item>
      <title>Delete CFS Sessions</title>
      <link>/docs-csm/en-12/operations/configuration_management/delete_cfs_sessions/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:16 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/delete_cfs_sessions/</guid>
      <description>Delete CFS Sessions Delete an existing Configuration Framework Service (CFS) configuration session with the CFS delete command.
Prerequisites This requires that the Cray command line interface is configured. See Configure the Cray Command Line Interface.
Delete single CFS session Use the session name to delete the session:
ncn# cray cfs sessions delete &amp;lt;session_name&amp;gt; No output is expected.
Delete multiple CFS sessions To delete all completed CFS sessions, use the deleteall command.</description>
    </item>
    
    <item>
      <title>Ansible Execution Environments</title>
      <link>/docs-csm/en-12/operations/configuration_management/ansible_execution_environments/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/ansible_execution_environments/</guid>
      <description>Ansible Execution Environments Configuration Framework Service (CFS) sessions are comprised of a single Kubernetes pod with several containers. Inventory and Git clone setup containers run first, and a teardown container runs last (if the session is running an image customization).
The containers that run the Ansible code cloned from the Git repositories in the configuration layers are Ansible Execution Environments (AEE). The AEE is provided as a SLES-based docker image, which includes Ansible version 2.</description>
    </item>
    
    <item>
      <title>Ansible Inventory</title>
      <link>/docs-csm/en-12/operations/configuration_management/ansible_inventory/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/ansible_inventory/</guid>
      <description>Ansible Inventory The Configuration Framework Service (CFS) provides several options for targeting nodes or boot images for configuration by Ansible. The contents of the Ansible inventory determine which nodes are available for configuration in each CFS session and how default configuration values can be customized. For more information on what it means to define an inventory, see Specifying Hosts and Groups.
The following are the inventory options provided by CFS:</description>
    </item>
    
    <item>
      <title>Automatic Session Deletion With Sessionttl</title>
      <link>/docs-csm/en-12/operations/configuration_management/automatic_session_deletion_with_sessionttl/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/automatic_session_deletion_with_sessionttl/</guid>
      <description>Automatic Session Deletion with sessionTTL By default, the Configuration Framework Service (CFS) will delete completed CFS sessions whose start date was more than seven days prior. Kubernetes jobs associated with these sessions will also be deleted as part of this process. This is done to ensure that CFS sessions do not accumulate and eventually adversely affect the performance of the Kubernetes cluster.
For larger systems or systems that do frequent reboots of nodes that are configured with CFS sessions, this setting may need to be reduced.</description>
    </item>
    
    <item>
      <title>CFS Flow</title>
      <link>/docs-csm/en-12/operations/configuration_management/cfs_flow_diagrams/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/cfs_flow_diagrams/</guid>
      <description>CFS Flow  Single Session Flow Automated Session Flow  Single Session Flow This section covers the components and actions taken when a user or service creates a session using the CFS sessions endpoint.
 A user creates a CFS configuration. A user creates a CFS session, causing a session record to be created. When a session record is created, the CFS-API also posts an event to a Kafka queue. The CFS-Operator is always monitoring the Kafka queue, and handles events as they come in.</description>
    </item>
    
    <item>
      <title>CFS Global Options</title>
      <link>/docs-csm/en-12/operations/configuration_management/cfs_global_options/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/cfs_global_options/</guid>
      <description>CFS Global Options The Configuration Framework Service (CFS) provides a global service options endpoint for modifying the base configuration of the service itself.
View the options with the following command:
ncn# cray cfs options list --format json Example output:
{ &amp;#34;additionalInventoryUrl&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;batchSize&amp;#34;: 25, &amp;#34;batchWindow&amp;#34;: 60, &amp;#34;batcherCheckInterval&amp;#34;: 10, &amp;#34;defaultAnsibleConfig&amp;#34;: &amp;#34;cfs-default-ansible-cfg&amp;#34;, &amp;#34;defaultBatcherRetryPolicy&amp;#34;: 1, &amp;#34;defaultPlaybook&amp;#34;: &amp;#34;site.yml&amp;#34;, &amp;#34;hardwareSyncInterval&amp;#34;: 10, &amp;#34;sessionTTL&amp;#34;: &amp;#34;7d&amp;#34; } The following are the CFS global options:
  additionalInventoryUrl
A Git clone URL to supply additional inventory content to all CFS sessions.</description>
    </item>
    
    <item>
      <title>CFS Key Management And Permission Denied Errors</title>
      <link>/docs-csm/en-12/operations/configuration_management/cfs_key_management/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/cfs_key_management/</guid>
      <description>CFS Key Management and Permission Denied Errors Configuration Framework Service (CFS) manages its own keys separate from keys for communication between CFS and the components or images that it is configuring. These are separate from the keys used by users and should not need to be managed.
Permission Denied Errors If Ansible is unable to connect with its target and fails with an Unreachable - Permission denied error, the first place to check is the cfs-state-reporter on the target node.</description>
    </item>
    
    <item>
      <title>Change The Ansible VerBOSity Logs</title>
      <link>/docs-csm/en-12/operations/configuration_management/change_the_ansible_verbosity_logs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/change_the_ansible_verbosity_logs/</guid>
      <description>Change the Ansible Verbosity Logs It is useful to view the Ansible logs in a Configuration Framework Session (CFS) session with greater verbosity than the default. CFS sessions are able to set the Ansible verbosity from the command line when the session is created. The verbosity will apply to all configuration layers in the session.
Specify an integer using the &amp;ndash;ansible-verbosity option, where 1 = -v, 2 = -vv, and so on.</description>
    </item>
    
    <item>
      <title>Configuration Layers</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_layers/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_layers/</guid>
      <description>Configuration Layers The Configuration Framework Service (CFS) uses configuration layers to specify the location of configuration content that will be applied. Configurations may include one or more layers. Each layer is defined by a Git repository clone URL, a Git commit, a name (optional), and the path in the repository to an Ansible playbook to execute.
Configurations with a single layer are useful when testing out a new configuration on targets, or when configuring system components with one product at a time.</description>
    </item>
    
    <item>
      <title>Configuration Management</title>
      <link>/docs-csm/en-12/operations/configuration_management/configuration_management/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/configuration_management/configuration_management/</guid>
      <description>Configuration Management The Configuration Framework Service (CFS) is available on systems for remote execution and configuration management of nodes and boot images. This includes nodes available in the Hardware State Manager (HSM) inventory (compute, management, and application nodes), and boot images hosted by the Image Management Service (IMS).
CFS configures nodes and images via a gitops methodology. All configuration content is stored in a version control service (VCS), and is managed by authorized system administrators.</description>
    </item>
    
    <item>
      <title>Upgrade Compute Nodes With CRUS</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:15 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/upgrade_compute_nodes_with_crus/</guid>
      <description>Upgrade Compute Nodes with CRUS Note: CRUS is deprecated in CSM 1.2.0. It will be removed in a future CSM release and replaced with BOS V2, which will provide similar functionality. See Deprecated features.
Upgrade a set of compute nodes with the Compute Rolling Upgrade Service (CRUS). Manage the workload management status of nodes and quiesce each node before taking the node out of service and upgrading it. Then reboot it into the upgraded state and return it to service within the workload manager (WLM).</description>
    </item>
    
    <item>
      <title>Compute Rolling Upgrades</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/compute_rolling_upgrades/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/compute_rolling_upgrades/</guid>
      <description>Compute Rolling Upgrades Note: CRUS is deprecated in CSM 1.2.0. It will be removed in a future CSM release and replaced with BOS V2, which will provide similar functionality. See Deprecated features.
The Compute Rolling Upgrade Service (CRUS) upgrades sets of compute nodes without requiring an entire set of nodes to be out of service at once. CRUS manages the workload management status of nodes, handling each of the following steps required to upgrade compute nodes:</description>
    </item>
    
    <item>
      <title>CRUS Workflow</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/crus_workflow/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/crus_workflow/</guid>
      <description>CRUS Workflow Note: CRUS is deprecated in CSM 1.2.0. It will be removed in a future CSM release and replaced with BOS V2, which will provide similar functionality. See Deprecated features.
The following workflow is intended to be a high-level overview of how to upgrade compute nodes. This workflow depicts how services interact with each other during the compute node upgrade process, and helps to provide a quicker and deeper understanding of how the system functions.</description>
    </item>
    
    <item>
      <title>Troubleshoot A Failed CRUS Session Because Of Bad Parameters</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_bad_parameters/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Bad Parameters Note: CRUS is deprecated in CSM 1.2.0. It will be removed in a future CSM release and replaced with BOS V2, which will provide similar functionality. See Deprecated features.
A CRUS session must be deleted and recreated if it does not start or complete because of parameters having incorrect values.
The following are examples of incorrect parameters:
 Choosing the wrong Boot Orchestration Service (BOS) session template.</description>
    </item>
    
    <item>
      <title>Troubleshoot A Failed CRUS Session Because Of Unmet Conditions</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_a_failed_crus_session_due_to_unmet_conditions/</guid>
      <description>Troubleshoot a Failed CRUS Session Because of Unmet Conditions Note: CRUS is deprecated in CSM 1.2.0. It will be removed in a future CSM release and replaced with BOS V2, which will provide similar functionality. See Deprecated features.
If a CRUS session has any unmet conditions, adding or fixing them will cause the session to continue from wherever it got stuck. Updating other parts of the system to meet the required conditions of a CRUS session will unblock the upgrade session.</description>
    </item>
    
    <item>
      <title>Troubleshoot Nodes Failing To Upgrade In A CRUS Session</title>
      <link>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/compute_rolling_upgrades/troubleshoot_nodes_failing_to_upgrade_in_a_crus_session/</guid>
      <description>Troubleshoot Nodes Failing to Upgrade in a CRUS Session Note: CRUS is deprecated in CSM 1.2.0. It will be removed in a future CSM release and replaced with BOS V2, which will provide similar functionality. See Deprecated features.
Troubleshoot compute nodes failing to upgrade during a Compute Rolling Upgrade Service (CRUS) session and rerun the session on the failed nodes.
When nodes are marked as failed they are added to the failed node group associated with the upgrade session, and the nodes are marked as down in the workload manager (WLM).</description>
    </item>
    
    <item>
      <title>Troubleshoot UAN Boot Issues</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_uan_boot_issues/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_uan_boot_issues/</guid>
      <description>Troubleshoot UAN Boot Issues Use this topic to guide troubleshooting of UAN boot issues.
The UAN boot process BOS boots UANs. BOS uses session templates to define various parameters such as:
 Which nodes to boot Which image to boot Kernel parameters Whether to perform post-boot configuration (Node Personalization) of the nodes by CFS. Which CFS configuration to use if Node Personalization is enabled.  UAN boots are performed in three phases:</description>
    </item>
    
    <item>
      <title>Upload Node Boot Information To Boot Script Service (bss)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/upload_node_boot_information_to_boot_script_service_bss/</guid>
      <description>Upload Node Boot Information to Boot Script Service (BSS) The following information must be uploaded to BSS as a prerequisite to booting a node via iPXE:
 The location of an initrd image in the artifact repository The location of a kernel image in the artifact repository Kernel boot parameters The node(s) associated with that information, using either host name or NID  BSS manages the iPXE boot scripts that coordinate the boot process for nodes, and it enables basic association of boot scripts with nodes.</description>
    </item>
    
    <item>
      <title>View The Status Of A BOS Session</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/view_the_status_of_a_bos_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:14 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/view_the_status_of_a_bos_session/</guid>
      <description>View the Status of a BOS Session The Boot Orchestration Service (BOS) supports a status endpoint that reports the status for individual BOS sessions. The status can be retrieved for each boot set within the session, as well as the individual items within a boot set.
BOS sessions contain one or more boot sets. Each boot set contains one or more phases, depending upon the operation for that session. For example, a reboot operation would have a shutdown, boot, and possibly configuration phase, but a shutdown operation would only have a shutdown phase.</description>
    </item>
    
    <item>
      <title>BOS Sessions</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/sessions/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/sessions/</guid>
      <description>BOS Sessions Overview of Boot Orchestration Service (BOS) session operations and limitations.
BOS creates a session when it is asked to perform one of the following operations:
 Boot - Boot a designated collection of nodes. Shutdown - Shutdown a designated collection of nodes. Reboot - Reboot a designated collection of nodes. Configure - Configure a designated collection of booted nodes.  BOS sessions can be used to boot compute nodes with customized image roots.</description>
    </item>
    
    <item>
      <title>Stage Changes Without BOS</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/stage_changes_without_bos/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/stage_changes_without_bos/</guid>
      <description>Stage Changes Without BOS Sometimes there is a need to stages changes to take place on a reboot, without immediately rebooting a node. When this is called for, users can bypass BOS, and set boot artifacts or configuration that will only take place when a node is later booted, whether that occurs manually, or triggered by a task manager.
Stage Boot Artifacts For information on staging boot artifacts, see the section Upload Node Boot Information to Boot Script Service (BSS).</description>
    </item>
    
    <item>
      <title>Tools For Resolving Compute Node Boot Issues</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/tools_for_resolving_boot_issues/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/tools_for_resolving_boot_issues/</guid>
      <description>Tools for Resolving Compute Node Boot Issues A number of tools can be used to analyze and debug issues encountered during the compute node boot process. The underlying issue and symptoms dictate the type of tool required.
nmap Use nmap to send out DHCP discover requests to test DHCP. nmap can be installed using the following command:
ncn# zypper install nmap To reach the DHCP server, the request generally needs to be sent over the Node Management network (NMN) from a non-compute node (NCN).</description>
    </item>
    
    <item>
      <title>Troubleshoot Booting Nodes With Hardware Issues</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_booting_nodes_with_hardware_issues/</guid>
      <description>Troubleshoot Booting Nodes with Hardware Issues How to identify a node with hardware issues and how to disable is via the HSM.
If a node included in a Boot Orchestration Service (BOS) session template is having hardware issues, it can prevent the node from powering back up correctly. The entire BOS session will fail with a timeout error waiting for the node to become ready.
The following is example log output from a node with hardware issues, resulting in a failed BOS session:</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Dynamic Host Configuration Protocol (DHCP)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_dynamic_host_configuration_protocol_dhcp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Dynamic Host Configuration Protocol (DHCP) DHCP issues can result in node boot failures. This procedure helps investigate and resolve such issues.
Prerequisites  This procedure requires administrative privileges. kubectl is installed.  Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
Procedure   Log in to a non-compute node (NCN) as root.
  Check that the DHCP service is running.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Slow Boot Times</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_slow_boot_times/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Slow Boot Times Inspect BOS, the Boot Orchestration Agent (BOA) job logs, and the Configuration Framework Service (CFS) job logs to obtain information that is critical for boot troubleshooting. Use this procedure to determine why compute nodes are booting slower than expected.
Prerequisites A boot session has been created with the Boot Orchestration Service (BOS).
Procedure   View the BOA logs.
  Find the BOA job from the boot session.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To The Boot Script Service (bss)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_the_boot_script_service_bss/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to the Boot Script Service (BSS) Boot Script Service (BSS) delivers a boot script to a node based on its MAC address. This boot script tells the node where to obtain its boot artifacts, which include:
 kernel initrd  In addition, the boot script also contains the kernel boot parameters. This procedure helps resolve issues related to missing boot artifacts.
Prerequisites This procedure requires administrative privileges.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Trivial File Transfer Protocol (tftp)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_trivial_file_transfer_protocol_tftp/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Trivial File Transfer Protocol (TFTP) TFTP issues can result in node boot failures. Use this procedure to investigate and resolve such issues.
Prerequisites This procedure requires administrative privileges.
Limitations Encryption of compute node logs is not enabled, so the passwords may be passed in clear text.
  Log onto a non-compute node (NCN) as root.
  Check that the TFTP service is running.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Related To Unified Extensible Firmware Interface (uefi)</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_related_to_unified_extensible_firmware_interface_uefi/</guid>
      <description>Troubleshoot Compute Node Boot Issues Related to Unified Extensible Firmware Interface (UEFI) If a node is stuck in the UEFI shell, ConMan will be able to connect to it, but nothing else will appear in its logs. The node&amp;rsquo;s logs will look similar to the following, indicating that ConMan is updating its log hourly:
&amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 20:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 21:00:00 CDT. &amp;lt;ConMan&amp;gt; Console [86] log at 2018-09-07 22:00:00 CDT.</description>
    </item>
    
    <item>
      <title>Troubleshoot Compute Node Boot Issues Using Kubernetes</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:13 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/troubleshoot_compute_node_boot_issues_using_kubernetes/</guid>
      <description>Troubleshoot Compute Node Boot Issues Using Kubernetes A number of Kubernetes commands can be used to debug issues related to the node boot process. All of the traffic bound for the DHCP server, TFTP server, and Boot Script Service (BSS) is sent on the Node Management Network (NMN).
In the current arrangement, all three services are located on a non-compute node (NCN). Thus, traffic must first travel through the NCN to reach these services inside their pods.</description>
    </item>
    
    <item>
      <title>BOS Limitations For Gigabyte BMC Hardware</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/limitations_for_gigabyte_bmc_hardware/</guid>
      <description>BOS Limitations for Gigabyte BMC Hardware Special steps need to be taken when using the Boot Orchestration Service (BOS) to boot, reboot, shutdown, or configure Gigabyte hardware. Gigabyte hardware treats power off and power on requests as successful, regardless of if actually successfully completed. The power on/off requests are ignored by Cray Advanced Platform Monitoring and Control (CAPMC) if they are received within a short period of time, which is typically around 60 seconds per operation.</description>
    </item>
    
    <item>
      <title>BOS Session Templates</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/session_templates/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/session_templates/</guid>
      <description>BOS Session Templates Describes the contents of a BOS session template.
A session template can be created by specifying parameters as part of the call to the Boot Orchestration Service (BOS). When calling BOS directly, JSON is passed as part of the call.
Session templates can be used to boot images that are customized with the Image Management Service (IMS). A session template has a collection of one or more boot set objects.</description>
    </item>
    
    <item>
      <title>Kernel Boot Parameters</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/kernel_boot_parameters/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/kernel_boot_parameters/</guid>
      <description>Kernel Boot Parameters The Image Management Service (IMS) extracts kernel boot parameters from the /boot/kernel-parameters file in the image, if that file exists, and stores them in S3. IMS already stores the other boot artifacts (kernel, initrd, and rootfs) in S3. When told to boot an image, the Boot Orchestration Service (BOS) will extract these parameters and deliver them to the Boot Script Service (BSS) so they can be used during the next boot of a node.</description>
    </item>
    
    <item>
      <title>Limit The Scope Of A BOS Session</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/limit_the_scope_of_a_bos_session/</guid>
      <description>Limit the Scope of a BOS Session The Boot Orchestration Service (BOS) supports an optional &amp;ndash;limit parameter when creating a session. This parameter can be used to further limit the nodes that BOS runs against, and is applied to all boot sets.
The --limit parameter takes a comma-separated list of nodes, groups, or roles in any combination. The BOS session will be limited to run against components that match both the boot set information and one or more of the nodes, groups, or roles listed in the limit.</description>
    </item>
    
    <item>
      <title>Log File Locations And Ports Used In Compute Node Boot Troubleshooting</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/log_file_locations_and_ports_used_in_compute_node_boot_troubleshooting/</guid>
      <description>Log File Locations and Ports Used in Compute Node Boot Troubleshooting This section includes the port IDs and log file locations of components associated with the node boot process.
Log File Locations The log file locations for ConMan, DHCP, and TFTP.
  ConMan logs are located within the conman pod at /var/log/conman.log.
  DHCP:
ncn-m001# kubectl logs DHCP_POD_ID   TFTP:
ncn-m001# kubectl logs -n services TFTP_POD_ID   Port IDs The following table includes the port IDs for DHCP and TFTP.</description>
    </item>
    
    <item>
      <title>Manage A BOS Session</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/manage_a_bos_session/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/manage_a_bos_session/</guid>
      <description>Manage a BOS Session Once there is a Boot Orchestration Service (BOS) session template created, users can perform operations on nodes, such as boot, reboot, configure, and shutdown. Managing sessions through the Cray CLI can be accomplished using the cray bos session commands.
Create a New Session Creating a new BOS session requires the following command-line options:
 --template-uuid: Use this option to specify the name value returned in the cray bos sessiontemplate list command.</description>
    </item>
    
    <item>
      <title>Manage A Session Template</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/manage_a_session_template/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/manage_a_session_template/</guid>
      <description>Manage a Session Template A session template must be created before starting a session with the Boot Orchestration Service (BOS). Session templates are managed via the Cray CLI with the cray bos sessiontemplate commands.
Get the Framework for a Session Template When creating a new BOS session template, it can be helpful to start with a framework and then edit it as needed. Use the following command to retrieve the BOS session template framework:</description>
    </item>
    
    <item>
      <title>Node Boot Root Cause Analysis</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/node_boot_root_cause_analysis/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/node_boot_root_cause_analysis/</guid>
      <description>Node Boot Root Cause Analysis The first step in debugging compute node boot-related issues is to determine the underlying cause, and the stage that the issue was encountered at.
The ConMan tool collects compute node logs. To learn more about ConMan, refer to ConMan.
A node&amp;rsquo;s console data can be accessed through its log file, as described in Access Compute Node Logs). This information can also be accessed by connecting to the node&amp;rsquo;s console with ipmitool.</description>
    </item>
    
    <item>
      <title>Redeploy The IPXE And Tftp Services</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:12 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/redeploy_the_ipxe_and_tftp_services/</guid>
      <description>Redeploy the iPXE and TFTP Services Redeploy the iPXE and TFTP services if a pod with a ceph-fs Process Virtualization Service (PVS) on a Kubernetes worker node is causing a HEALTH_WARN error.
Resolve issues with ceph-fs and ceph-mds by restarting the iPXE and TFTP services. The Ceph cluster will return to a healthy state after this procedure.
Prerequisites This procedure requires administrative privileges.
Procedure   Find the iPXE and TFTP deployments.</description>
    </item>
    
    <item>
      <title>Clean Up Logs After A Boa Kubernetes Job</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/clean_up_logs_after_a_boa_kubernetes_job/</guid>
      <description>Clean Up Logs After a BOA Kubernetes Job Delete log entries from previous boot orchestration jobs. The Boot Orchestration Service (BOS) launches a Boot Orchestration Agent (BOA) Kubernetes job. BOA then launches a Configuration Framework Service (CFS) session, resulting in a CFS-BOA Kubernetes job. Thus, there are two separate sets of jobs that can be removed.
Deleting log entries creates more space and helps improve the usability of viewing logs.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Duplicate Address Warnings And Declined DHCP Offers In Logs</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_duplicate_address_warnings_and_declined_dhcp_offers_in_logs/</guid>
      <description>Compute Node Boot Issue Symptom: Duplicate Address Warnings and Declined DHCP Offers in Logs If the DHCP and node logs show duplicate address warnings and indicate declined DHCP offers, it may be because another component owns the IP address that DHCP is trying to assign to a node. If this happens, the node will not accept the IP address and will repeatedly submit a DHCP discover request. As a result, the node and DHCP become entangled in a loop of requesting and rejecting.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Message About Invalid Eeprom Checksum In Node Console Or Log</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_message_about_invalid_eeprom_checksum_in_node_console_or_log/</guid>
      <description>Compute Node Boot Issue Symptom: Message About Invalid EEPROM Checksum in Node Console or Log On rare occasions, the processor hardware may lose the Serial Over Lan (SOL) connections and may need to be reseated to allow the node to successfully boot.
Symptoms This issue can be identified if the following is displayed in the node&amp;rsquo;s console or log:
console.38:2018-09-08 04:54:51 [ 16.721165] ixgbe 0000:18:00.0: The EEPROM Checksum Is Not Valid console.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Node Is Not Able To Download The Required Artifacts</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_issue_symptom_node_is_not_able_to_download_the_required_artifacts/</guid>
      <description>Compute Node Boot Issue Symptom: Node is Not Able to Download the Required Artifacts If either or both of the kernel or the initrd boot artifacts are missing from the artifact repository, Boot Script Service (BSS), or both, the node will not be able to download the required boot artifacts and will fail to boot.
Symptoms The node&amp;rsquo;s console or log will display lines beginning with, &amp;lsquo;&#39;Could not start download&#39;&amp;rsquo;. Refer to the image below for an example of this error message.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Sequence</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_sequence/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/compute_node_boot_sequence/</guid>
      <description>Compute Node Boot Sequence Provides an overview of the compute node boot process and touches upon the fact that issues can be encountered during this process.
The following is a high-level overview of the boot sequence for compute nodes:
  The compute node is powered on.
  The BIOS issues a DHCP discover request.
  DHCP responds with the following:
 next-server, which is the IP address of the TFTP server.</description>
    </item>
    
    <item>
      <title>Configure The BOS Timeout When Booting Compute Nodes</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/configure_the_bos_timeout_when_booting_nodes/</guid>
      <description>Configure the BOS Timeout When Booting Compute Nodes Manually update the boa-job-template ConfigMap to tune the timeout and sleep intervals for the Boot Orchestration Agent (BOA). Correcting the timeout value is a good troubleshooting option for when BOS sessions hang waiting for nodes to be in a Ready state.
If the BOS timeout occurs when booting compute nodes, the system will be unable to boot via BOS.
Prerequisites A Boot Orchestration Service (BOS) session was run and compute nodes are failing to move to a Ready state.</description>
    </item>
    
    <item>
      <title>Create A Session Template To Boot Compute Nodes With Cps</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/create_a_session_template_to_boot_compute_nodes_with_cps/</guid>
      <description>Create a Session Template to Boot Compute Nodes with CPS When compute nodes are booted, the Content Projection Service (CPS) and Data Virtualization Service (DVS) project the root file system (rootfs) over the network to the compute nodes by default.
Another option when compute nodes are booted is to download their rootfs into RAM.
Procedure   Use either the cray bos session create CLI command or the bash script to create a session template.</description>
    </item>
    
    <item>
      <title>Edit The IPXE Embedded Boot Script</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/edit_the_ipxe_embedded_boot_script/</guid>
      <description>Edit the iPXE Embedded Boot Script Manually adjust the iPXE embedded boot script to change the order of network interfaces for DHCP request. Changing the order of network interfaces for DHCP requests helps improve boot time performance.
Prerequisites This procedure requires administrative privileges.
Procedure   Edit the ConfigMap using one of the following options.
NOTE: Save a backup of the ConfigMap before making any changes.
The following is an example of creating a backup:</description>
    </item>
    
    <item>
      <title>Healthy Compute Node Boot Process</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/healthy_compute_node_boot_process/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:11 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/healthy_compute_node_boot_process/</guid>
      <description>Healthy Compute Node Boot Process In order to investigate node boot-related issues, it is important to understand the flow of a healthy boot process and the associated components. This section outlines the normal flow of components that play a role in booting compute nodes, including DHCP, BSS, and TPTP.
DHCP A healthy DHCP exchange between server and client looks like the following:
   Traffic Description Sender     DHCP Discover A broadcast request from the client requesting an IP address.</description>
    </item>
    
    <item>
      <title>Boot Issue Symptom Node HSN Interface Does Not Appear Or Show Detected Links Detected</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_hsn_interface_does_not_appear_or_shows_no_link_detected/</guid>
      <description>Boot Issue Symptom: Node HSN Interface Does Not Appear or Show Detected Links Detected A node may fail to boot if the HSN interface is experiencing issues, or if it is not able to detect any links.
Symptom The node&amp;rsquo;s HSN interface does not appear in the output of the ip addr command or the output of the ethtool interface command shows no link detected.
Resolution Reseat the node&amp;rsquo;s PCIe card.</description>
    </item>
    
    <item>
      <title>Boot Orchestration</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_orchestration/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_orchestration/</guid>
      <description>Boot Orchestration The Boot Orchestration Service (BOS) is responsible for booting, configuring, and shutting down collections of nodes. This is accomplished using BOS components, such as boot orchestration session templates and sessions, as well as launching a Boot Orchestration Agent (BOA) that fulfills boot requests.
BOS users create a BOS session template via the REST API. A session template is a collection of metadata for a group of nodes and their desired boot artifacts and configuration.</description>
    </item>
    
    <item>
      <title>Boot UANs</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_uans/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_uans/</guid>
      <description>Boot UANs Boot UANs with an image so that they are ready for user logins.
Prerequisites UAN boot images and a BOS session template have been created. See Create UAN Boot Images.
Procedure   Create a BOS session to boot the UAN nodes.
ncn-mw# cray bos session create --template-uuid uan-sessiontemplate-PRODUCT_VERSION \  --operation reboot --format json | tee session.json Example output:
{ &amp;#34;links&amp;#34;: [ { &amp;#34;href&amp;#34;: &amp;#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d&amp;#34;, &amp;#34;jobId&amp;#34;: &amp;#34;boa-89680d0a-3a6b-4569-a1a1-e275b71fce7d&amp;#34;, &amp;#34;rel&amp;#34;: &amp;#34;session&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;GET&amp;#34; }, { &amp;#34;href&amp;#34;: &amp;#34;/v1/session/89680d0a-3a6b-4569-a1a1-e275b71fce7d/status&amp;#34;, &amp;#34;rel&amp;#34;: &amp;#34;status&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;GET&amp;#34; } ], &amp;#34;operation&amp;#34;: &amp;#34;reboot&amp;#34;, &amp;#34;templateUuid&amp;#34;: &amp;#34;uan-sessiontemplate-PRODUCT_VERSION&amp;#34; } The first attempt to reboot the UANs will most likely fail.</description>
    </item>
    
    <item>
      <title>BOS Workflows</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/bos_workflows/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/bos_workflows/</guid>
      <description>BOS Workflows The following workflows present a high-level overview of common Boot Orchestration Service (BOS) operations. These workflows depict how services interact with each other when booting, configuring, or shutting down nodes. They also help provide a quicker and deeper understanding of how the system functions.
The following workflows are included in this section:
 Boot and Configure Nodes Reconfigure Nodes Power Off Nodes  Boot and Configure Nodes Use Case: Administrator powers on and configures select compute nodes.</description>
    </item>
    
    <item>
      <title>Check The Progress Of BOS Session Operations</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/check_the_progress_of_bos_session_operations/</guid>
      <description>Check the Progress of BOS Session Operations Describes how to view the logs of BOS operations with Kubernetes.
When a Boot Orchestration Service (BOS) session is created, it will return a job ID. This ID can be used to locate the Boot Orchestration Agent (BOA) Kubernetes job that executes the session. For example:
ncn-m001# cray bos session create --template-uuid SESSIONTEMPLATE_NAME --operation Boot Example output:
operation = &amp;quot;Boot&amp;quot; templateUuid = &amp;quot;TEMPLATE_UUID&amp;quot; [[links]] href = &amp;quot;foo-c7faa704-3f98-4c91-bdfb-e377a184ab4f&amp;quot; jobId = &amp;quot;boa-a939bd32-9d27-433f-afc2-735e77ec8e58&amp;quot; rel = &amp;quot;session&amp;quot; type = &amp;quot;GET&amp;quot; All BOS Kubernetes pods operate in the services namespace.</description>
    </item>
    
    <item>
      <title>Clean Up After A BOS/boa Job Is Completed Or CANcelled</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/clean_up_after_a_bos-boa_job_is_completed_or_cancelled/</guid>
      <description>Clean Up After a BOS/BOA Job is Completed or Cancelled When a BOS session is created, there are a number of items created on the system. When a session is cancelled or completed, these items need to be cleaned up to ensure there is not lingering content from the session on the system.
When a session is launched, the items below are created:
 Boot Orchestration Agent (BOA) job: The Kubernetes job that runs and handles the BOS session.</description>
    </item>
    
    <item>
      <title>Compute Node Boot Issue Symptom Node Console Or Logs Indicate That The Server Response Has Timed Out</title>
      <link>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/boot_orchestration/boot_issue_symptom_node_console_or_logs_indicate_that_the_server_response_has_timed_out/</guid>
      <description>Compute Node Boot Issue Symptom: Node Console or Logs Indicate that the Server Response has Timed Out If the TFTP request is able to access the TFTP service pod but is unable to find its way back to the node, it may be because the kernel is not tracking established TFTP connections.
Symptoms The following image, which is tcpdump data from within the TFTP pod, shows what happens when the TFTP request cannot find a route back to the node that sent the request.</description>
    </item>
    
    <item>
      <title>Manage Artifacts With The Cray Cli</title>
      <link>/docs-csm/en-12/operations/artifact_management/manage_artifacts_with_the_cray_cli/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/manage_artifacts_with_the_cray_cli/</guid>
      <description>Manage Artifacts with the Cray CLI The artifacts (objects) available for use on the system are created and managed with the Cray CLI. The cray artifacts command provides the ability to manage any given artifact. The Cray CLI automatically authenticates users and provides Simple Storage Service (S3) credentials.
All operations with the cray artifacts command assume that the user has already been authenticated. If the user has not been authenticated with the Cray CLI, run the following command:</description>
    </item>
    
    <item>
      <title>Use S3 Libraries And Clients</title>
      <link>/docs-csm/en-12/operations/artifact_management/use_s3_libraries_and_clients/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:10 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/use_s3_libraries_and_clients/</guid>
      <description>Use S3 Libraries and Clients Several command line clients and language-specific libraries are available in addition to the Simple Storage Service (S3) RESTful API. Developers and system administrators can interact with artifacts in the S3 object store with these tools.
To learn more, refer to the following links:
 https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html - S3 Python client https://docs.aws.amazon.com/sdk-for-go/api/service/s3/ - Go client https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html - Amazon Web Services (AWS) S3 CLI  </description>
    </item>
    
    <item>
      <title>Artifact Management</title>
      <link>/docs-csm/en-12/operations/artifact_management/artifact_management/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/artifact_management/</guid>
      <description>Artifact Management The Ceph Object Gateway Simple Storage Service (S3) API is used for artifact management. The RESTful API that Ceph provides via the gateway is compatible with the basic data access model of the Amazon S3 API. See the https://docs.ceph.com/en/pacific/radosgw/s3/ for more information about compatibility. The object gateway is also referred to as the RADOS gateway or simply RGW.
S3 is an object storage service that provides high-level performance, scalability, security, and data availability.</description>
    </item>
    
    <item>
      <title>Generate Temporary S3 Credentials</title>
      <link>/docs-csm/en-12/operations/artifact_management/generate_temporary_s3_credentials/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:09 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/artifact_management/generate_temporary_s3_credentials/</guid>
      <description>Generate Temporary S3 Credentials Cray provides a simple token service (STS) via the API gateway for administrators to generate temporary Simple Storage Service (S3) credentials for use with S3 buckets. Temporary S3 credentials are generated using either cURL or Python.
The generated S3 credentials will expire after one hour.
Procedure   Retrieve temporary S3 credentials with cURL.
  Obtain a JWT token.
See Retrieve an Authentication Token for more information.</description>
    </item>
    
    <item>
      <title>Component Names (xnames)</title>
      <link>/docs-csm/en-12/operations/component_names_xnames/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/component_names_xnames/</guid>
      <description>Component Names (xnames) Component names (xnames) identify the geolocation for hardware components in the HPE Cray EX system. Every component is uniquely identified by these component names. Some, like the system cabinet number or the CDU number, can be changed by site needs. There is no geolocation encoded within the cabinet number, such as an X-Y coordinate system to relate to the floor layout of the cabinets. Other component names refer to the location within a cabinet and go down to the port on a card or switch or the socket holding a processor or a memory DIMM location.</description>
    </item>
    
    <item>
      <title>Configure Keycloak Account</title>
      <link>/docs-csm/en-12/operations/csm_product_management/configure_keycloak_account/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/configure_keycloak_account/</guid>
      <description>Configure Keycloak Account Installation of CSM software includes a default account for administrative access to keycloak.
Depending on choices made during the installation, there may be a federated connection to an external Identity Provider (IdP), such as an LDAP or AD server, which enables the use of external accounts in keycloak.
However, if the external accounts are not available, then an &amp;ldquo;internal user account&amp;rdquo; could be created in keycloak. Having a usable account in keycloak with administrative authorization enables the use of the cray CLI for many administrative commands, such as those used to Validate CSM Health and general operation of the management services via the API gateway.</description>
    </item>
    
    <item>
      <title>Configure Non-compute Nodes With CFS</title>
      <link>/docs-csm/en-12/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/configure_non-compute_nodes_with_cfs/</guid>
      <description>Configure Non-Compute Nodes with CFS Non-compute node (NCN) personalization applies post-boot configuration to the HPE Cray EX management nodes. Several HPE Cray EX product environments outside of CSM require NCN personalization to function. Consult the manual for each product to configure them on NCNs by referring to the HPE Cray EX System Software Getting Started Guide (S-8000) 22.07 on the HPE Customer Support Center.
This procedure defines the NCN personalization process for the CSM product using the Configuration Framework Service (CFS).</description>
    </item>
    
    <item>
      <title>Configure Packages With CFS</title>
      <link>/docs-csm/en-12/operations/csm_product_management/configure_csm_packages_with_cfs/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/configure_csm_packages_with_cfs/</guid>
      <description>Configure CSM packages with CFS Note: This section applies only to the 1.2.6 CSM release. Earlier versions of 1.2 do not include this playbook.
CSM includes a playbook that should be applied to Compute and Application node images. The csm_packages.yml playbook installs the packages for both the CFS and BOS reporters. These packages are necessary for CFS and BOS to run, so a configuration layer containing the playbook must be included in the image customization for any nodes that are expected to be managed with CFS and BOS.</description>
    </item>
    
    <item>
      <title>Perform NCN Personalization</title>
      <link>/docs-csm/en-12/operations/csm_product_management/perform_ncn_personalization/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/perform_ncn_personalization/</guid>
      <description>Perform NCN Personalization NCN personalization is the process of applying product-specific configuration to NCNs post-boot.
Prerequisites Prior to running this procedure, gather the following information required by CFS to create a configuration layer:
 HTTP clone URL for the configuration repository in VCS Path to the Ansible play to run in the repository Commit ID in the repository for CFS to pull and run on the nodes  Products may supply multiple plays to run, in which case multiple configuration layers must be created.</description>
    </item>
    
    <item>
      <title>Post-install Customizations</title>
      <link>/docs-csm/en-12/operations/csm_product_management/post_install_customizations/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/post_install_customizations/</guid>
      <description>Post-Install Customizations Post-install customizations may be needed as systems scale. These customizations also need to persist across future installs or upgrades. Not all resources can be customized post-install; common scenarios are documented in the following sections.
The following is a guide for determining where issues may exist, how to adjust the resources, and how to ensure the changes will persist. Different values may be be needed for systems as they scale.</description>
    </item>
    
    <item>
      <title>Remove Artifacts From Product Installations</title>
      <link>/docs-csm/en-12/operations/csm_product_management/remove_artifacts_from_product_installations/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/remove_artifacts_from_product_installations/</guid>
      <description>Remove Artifacts from Product Installations Remove product artifacts that were imported from various Cray products. These instructions provide guidance for removing Image Management Service (IMS) images, IMS recipes, and Git repositories present in the Cray Product Catalog from the system.
The examples in this procedure show how to remove the product artifacts for the Cray System Management (CSM) product.
WARNING: If individual Cray products have removal procedures, those instructions supersede this procedure.</description>
    </item>
    
    <item>
      <title>Validate Signed Rpms</title>
      <link>/docs-csm/en-12/operations/csm_product_management/validate_signed_rpms/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:01 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/validate_signed_rpms/</guid>
      <description>Validate Signed RPMs The HPE Cray EX system signs RPMs to provide an extra level of security. Use the following procedure to import a key from either My HPE Software Center or a Kubernetes Secret, and then use that key to validate the RPM package signatures on each node type.
The RPMs will vary on compute, application, worker, master, and storage nodes. Check each node type to ensure the RPMs are correctly signed.</description>
    </item>
    
    <item>
      <title>Accessing Livecd Usb Device After Reboot</title>
      <link>/docs-csm/en-12/operations/access_livecd_usb_device_after_reboot/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/access_livecd_usb_device_after_reboot/</guid>
      <description>Accessing LiveCD USB Device After Reboot This is a procedure that only applies to the LiveCD USB device after the PIT node has been rebooted.
 USB ONLY If the installation above was done from a Remote ISO.
 After deploying the LiveCD&amp;rsquo;s NCN, the LiveCD USB itself is unharmed and available to an administrator.
Procedure   Mount and view the USB device.
ncn-m001# mkdir -pv /mnt/{cow,pitdata} ncn-m001# mount -vL cow /mnt/cow ncn-m001# mount -vL PITDATA /mnt/pitdata ncn-m001# ls -ld /mnt/cow/rw/* Example output:</description>
    </item>
    
    <item>
      <title>Change Passwords And Credentials</title>
      <link>/docs-csm/en-12/operations/csm_product_management/change_passwords_and_credentials/</link>
      <pubDate>Fri, 08 Jul 2022 03:30:00 +0000</pubDate>
      
      <guid>/docs-csm/en-12/operations/csm_product_management/change_passwords_and_credentials/</guid>
      <description>Change Passwords and Credentials This is an overarching procedure to change all credentials managed by Cray System Management (CSM) in HPE Cray EX system to new values.
There are many passwords and credentials used in different contexts to manage the system. These can be changed as needed. Their initial settings are documented, so it is recommended to change them during or soon after a CSM software installation.
Prerequisites  Review procedures in Manage System Passwords.</description>
    </item>
    
  </channel>
</rss>
